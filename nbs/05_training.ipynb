{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4937e64-3ce6-4ec8-9d8e-7f05e9ba3dba",
   "metadata": {},
   "source": [
    "# Training our foundational model\n",
    "\n",
    "> \"Lets start training!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e4705-2246-4933-bace-9cdf64adeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6ab3e-2354-4aef-b44f-e9c6ae451e8a",
   "metadata": {},
   "source": [
    "## Setup, indexing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8df81-dadc-4661-ae90-47e31f0de989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages/Bio/__init__.py:138: BiopythonWarning: You may be importing Biopython from inside the source tree. This is bad practice and might lead to downstream issues. In particular, you might encounter ImportErrors due to missing compiled C extensions. We recommend that you try running your code from outside the source tree. If you are outside the source tree then you have a setup.py file in an unexpected directory: /home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from llm_mito_scanner.analysis.training import get_training_annotation_paths\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50d407-6de9-4d35-97e4-ec94a635ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from yaml import safe_load\n",
    "\n",
    "tqdm.pandas(ncols=80, leave=False)\n",
    "\n",
    "with open(\"../config.yml\") as f:\n",
    "    config = safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22252b2-60fc-43d2-a616-a7e1df49fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "data_path = Path(config.get(\"data_path\"))\n",
    "training_data_path = data_path / \"training\"\n",
    "training_index_path = data_path / \"training_index.csv\"\n",
    "gene_to_protein_maps_path = data_path / \"gene_to_protein_maps.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df0cf8-326f-4979-b109-b2424f4051a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import json\n",
    "\n",
    "if not training_index_path.exists() or not gene_to_protein_maps_path.exists():\n",
    "    gene_to_protein_maps, training_paths = get_training_annotation_paths(training_data_path)\n",
    "    training_paths.to_csv(training_index_path, index=False)\n",
    "    gene_to_protein_maps.to_csv(gene_to_protein_maps_path)\n",
    "else:\n",
    "    training_paths = pd.read_csv(training_index_path)\n",
    "    gene_to_protein_maps = pd.read_csv(gene_to_protein_maps_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b16ce-590a-4d3c-8c27-77f891a52dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>gene</th>\n",
       "      <th>gene_annotation</th>\n",
       "      <th>protein_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation       gene                                    gene_annotation   \n",
       "0  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \\\n",
       "1  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "2  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "3  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "4  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "\n",
       "                                  protein_annotation  \n",
       "0  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "1  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "2  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "3  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "4  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "training_paths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2c355-4ffc-4134-aeb5-74a29207f92b",
   "metadata": {},
   "source": [
    "## Build the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9529859-4c8a-4ddd-a14b-62f71ea979de",
   "metadata": {},
   "source": [
    "### Construct the tokenizer, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93626513-9c6d-4c2b-9ae4-9104f0c64031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from fastai.text.core import BaseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba19f8-b4b7-4dcd-b702-d13b86fd8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tokenizer = BaseTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53ef94-1ce6-4003-ba1b-7dc794bccf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def yield_file_tokens(path: Path, chunksize=1024, sep=\"\\s\"):\n",
    "    with path.open('r') as f:\n",
    "        row = ''\n",
    "        while (chunk := f.read(chunksize)) != '':   # End of file\n",
    "            while (i := chunk.find(sep)) != -1:     # No separator found\n",
    "                yield row + chunk[:i]\n",
    "                chunk = chunk[i+1:]\n",
    "                row = ''\n",
    "            row += chunk\n",
    "        yield row\n",
    "\n",
    "\n",
    "def yield_all_training_tokens(file_paths: list[Path]):\n",
    "    file_path_tqdm = tqdm(file_paths)\n",
    "    return map(lambda path: yield_file_tokens(path), file_path_tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e08ff-7b7d-4517-8145-dc018787d4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106119"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file_tokens = yield_file_tokens(Path(training_paths.gene_annotation.iloc[0]))\n",
    "len(next(test_file_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be241a-332c-4b26-8436-797843df5890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123138"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "all_annotation_file_paths = [Path(p) for p in training_paths.gene_annotation.dropna().unique().tolist() + \\\n",
    "    training_paths.protein_annotation.dropna().unique().tolist()]\n",
    "len(all_annotation_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7bdc6-1e09-435c-876f-34f181d7cb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Path('/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000004.12/254251/rna-NM_001394446.1.txt'),\n",
       " Path('/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000004.12/254251/rna-NM_153686.8.txt')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_annotation_file_paths[26424:26424+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14992ac9-884b-4740-a763-c145e4c843e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6a60ad4a7540afaa848d4395fc8c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_iter = yield_all_training_tokens(\n",
    "    all_annotation_file_paths\n",
    ")\n",
    "vocab = build_vocab_from_iterator(train_iter, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e06c7e-a0bd-4553-b26d-e94f6f936b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a5f46-1270-4753-9fef-7e893963d1f3",
   "metadata": {},
   "source": [
    "### Build our training, validation, test idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c85be0-f3c4-42db-b40e-895a78cbf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Build indices for train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898ad26-f536-4259-987b-37406da51d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9946d-8338-4b74-bc99-d18050b00447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc009b-0f0a-4364-9530-68a2784f8c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5113515-92c2-4825-b7f6-5f972384d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
