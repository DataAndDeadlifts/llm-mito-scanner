{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4937e64-3ce6-4ec8-9d8e-7f05e9ba3dba",
   "metadata": {},
   "source": [
    "# Training our foundational model\n",
    "\n",
    "> \"Lets start training!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e4705-2246-4933-bace-9cdf64adeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6ab3e-2354-4aef-b44f-e9c6ae451e8a",
   "metadata": {},
   "source": [
    "## Setup, indexing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8df81-dadc-4661-ae90-47e31f0de989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages/Bio/__init__.py:138: BiopythonWarning: You may be importing Biopython from inside the source tree. This is bad practice and might lead to downstream issues. In particular, you might encounter ImportErrors due to missing compiled C extensions. We recommend that you try running your code from outside the source tree. If you are outside the source tree then you have a setup.py file in an unexpected directory: /home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from llm_mito_scanner.analysis.training import get_training_annotation_paths\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50d407-6de9-4d35-97e4-ec94a635ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from yaml import safe_load\n",
    "\n",
    "tqdm.pandas(ncols=80, leave=False)\n",
    "\n",
    "with open(\"../config.yml\") as f:\n",
    "    config = safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22252b2-60fc-43d2-a616-a7e1df49fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "data_path = Path(config.get(\"data_path\"))\n",
    "training_data_path = data_path / \"training\"\n",
    "training_index_path = data_path / \"training_index.csv\"\n",
    "gene_to_protein_maps_path = data_path / \"gene_to_protein_maps.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df0cf8-326f-4979-b109-b2424f4051a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import json\n",
    "\n",
    "if not training_index_path.exists() or not gene_to_protein_maps_path.exists():\n",
    "    gene_to_protein_maps, training_paths = get_training_annotation_paths(training_data_path)\n",
    "    training_paths.to_csv(training_index_path, index=False)\n",
    "    gene_to_protein_maps.to_csv(gene_to_protein_maps_path)\n",
    "else:\n",
    "    training_paths = pd.read_csv(training_index_path)\n",
    "    gene_to_protein_maps = pd.read_csv(gene_to_protein_maps_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b16ce-590a-4d3c-8c27-77f891a52dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>gene</th>\n",
       "      <th>gene_annotation</th>\n",
       "      <th>protein_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation       gene                                    gene_annotation   \n",
       "0  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \\\n",
       "1  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "2  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "3  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "4  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "\n",
       "                                  protein_annotation  \n",
       "0  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "1  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "2  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "3  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "4  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "training_paths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2c355-4ffc-4134-aeb5-74a29207f92b",
   "metadata": {},
   "source": [
    "## Build the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9529859-4c8a-4ddd-a14b-62f71ea979de",
   "metadata": {},
   "source": [
    "### Construct the tokenizer, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93626513-9c6d-4c2b-9ae4-9104f0c64031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from fastai.text.core import BaseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba19f8-b4b7-4dcd-b702-d13b86fd8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tokenizer = BaseTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be241a-332c-4b26-8436-797843df5890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123138"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "all_annotation_file_paths = [Path(p) for p in training_paths.gene_annotation.dropna().unique().tolist() + \\\n",
    "    training_paths.protein_annotation.dropna().unique().tolist()]\n",
    "len(all_annotation_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc1b0b-f9e6-4b23-bba7-3a106c305c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTER: 0\n",
      "LEFTOVER: \n",
      "CHUNK: [N]A [N]C [N]A [N]T [N]C [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]T [N]C [N]C [N]T [N]T [N]T [N]G [N]G [N]G [N]G [N]C [N]A [N]T [N]C [N]T [N]C [N]T [N]G [N]T [N]C [N]A [N]T [N]G [N]T [N]G [N]C [N]T [N]T [N]A [N]T [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]C [N]C [N]T [N]C [N]T [N]C [N]C [N]A [N]T [N]C [N]T [N]A [N]T [N]G [N]T [N]T [N]A [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]C [N]T [N]T [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]G [N]C [N]C [N]T [N]C [N]T [N]T [N]T [N]C [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]G [N]C [N]T [N]T [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]A [N]A [N]T [N]T [N]T [N]C [N]C [N]A [N]A [N]C [N]T [N]G [N]C [N]T [N]C [N]A [N]A [N]C [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]A [N]T [N]G [N]G [N]A [N]C [N]A [N]A [N]A [N]C [N]C [N]G [N]C [N]C [N]C [N]C [N]T [N]C [N]A [N]T [N]A [N]T [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]C [N]A [N]T [N]G [N]A [N]A [N]C [N]T [N]G [N]G [N]A [N]G [N]G [N]A [N]G [N]G [N]G [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G [N]A\n",
      "LAST SEP: 1019\n",
      "TOKEN CHUNK: [N]A [N]C [N]A [N]T [N]C [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]T [N]C [N]C [N]T [N]T [N]T [N]G [N]G [N]G [N]G [N]C [N]A [N]T [N]C [N]T [N]C [N]T [N]G [N]T [N]C [N]A [N]T [N]G [N]T [N]G [N]C [N]T [N]T [N]A [N]T [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]C [N]C [N]T [N]C [N]T [N]C [N]C [N]A [N]T [N]C [N]T [N]A [N]T [N]G [N]T [N]T [N]A [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]C [N]T [N]T [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]G [N]C [N]C [N]T [N]C [N]T [N]T [N]T [N]C [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]G [N]C [N]T [N]T [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]A [N]A [N]T [N]T [N]T [N]C [N]C [N]A [N]A [N]C [N]T [N]G [N]C [N]T [N]C [N]A [N]A [N]C [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]A [N]T [N]G [N]G [N]A [N]C [N]A [N]A [N]A [N]C [N]C [N]G [N]C [N]C [N]C [N]C [N]T [N]C [N]A [N]T [N]A [N]T [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]C [N]A [N]T [N]G [N]A [N]A [N]C [N]T [N]G [N]G [N]A [N]G [N]G [N]A [N]G [N]G [N]G [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G\n",
      "CHUNK TOKENS: ['[N]A', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]T', '[N]G', '[N]T', '[N]C', '[N]C', '[N]T', '[N]T', '[N]T', '[N]G', '[N]G', '[N]G', '[N]G', '[N]C', '[N]A', '[N]T', '[N]C', '[N]T', '[N]C', '[N]T', '[N]G', '[N]T', '[N]C', '[N]A', '[N]T', '[N]G', '[N]T', '[N]G', '[N]C', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]G', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]T', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]T', '[N]C', '[N]T', '[N]A', '[N]T', '[N]G', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]T', '[N]C', '[N]T', '[N]T', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]G', '[N]C', '[N]C', '[N]T', '[N]C', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]G', '[N]C', '[N]T', '[N]T', '[N]T', '[N]G', '[N]T', '[N]A', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]C', '[N]A', '[N]A', '[N]C', '[N]C', '[N]T', '[N]T', '[N]T', '[N]C', '[N]T', '[N]G', '[N]A', '[N]T', '[N]G', '[N]G', '[N]A', '[N]C', '[N]A', '[N]A', '[N]A', '[N]C', '[N]C', '[N]G', '[N]C', '[N]C', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]T', '[N]A', '[N]T', '[N]C', '[N]T', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]G', '[N]A', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]G', '[N]A', '[N]G', '[N]G', '[N]A', '[N]G', '[N]G', '[N]G', '[N]G', '[N]A', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]G']\n",
      "COUNTER: 1\n",
      "LEFTOVER:  [N]A\n",
      "CHUNK:  [N]C [N]T [N]A [N]G [N]G [N]T [N]G [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]T [N]G [N]C [N]A [N]T [N]A [N]G [N]C [N]T [N]G [N]G [N]A [N]G [N]A [N]C [N]T [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]G [N]G [N]T [N]A [N]G [N]G [N]T [N]T [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]G [N]G [N]G [N]G [N]A [N]C [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]A [N]G [N]G [N]G [N]G [N]T [N]G [N]T [N]T [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]T [N]A [N]A [N]A [N]C [N]C [N]C [N]A [N]A [N]A [N]A [N]G [N]A [N]C [N]T [N]G [N]T [N]T [N]T [N]C [N]A [N]G [N]A [N]C [N]A [N]G [N]T [N]A [N]T [N]G [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]A [N]C [N]A [N]C [N]T [N]G [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]T [N]A [N]T [N]C [N]A [N]G [N]G [N]T [N]T [N]C [N]A [N]C [N]T [N]G [N]C [N]T [N]G [N]G [N]C [N]C [N]A [N]C [N]A [N]G [N]A [N]A [N]C [N]C [N]A [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]T [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]T [N]G [N]C [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]A [N]A [N]T [N]C [N]A [N]A [N]T [N]\n",
      "LAST SEP: 1020\n",
      "TOKEN CHUNK:  [N]A [N]C [N]T [N]A [N]G [N]G [N]T [N]G [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]T [N]G [N]C [N]A [N]T [N]A [N]G [N]C [N]T [N]G [N]G [N]A [N]G [N]A [N]C [N]T [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]G [N]G [N]T [N]A [N]G [N]G [N]T [N]T [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]G [N]G [N]G [N]G [N]A [N]C [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]A [N]G [N]G [N]G [N]G [N]T [N]G [N]T [N]T [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]T [N]A [N]A [N]A [N]C [N]C [N]C [N]A [N]A [N]A [N]A [N]G [N]A [N]C [N]T [N]G [N]T [N]T [N]T [N]C [N]A [N]G [N]A [N]C [N]A [N]G [N]T [N]A [N]T [N]G [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]A [N]C [N]A [N]C [N]T [N]G [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]T [N]A [N]T [N]C [N]A [N]G [N]G [N]T [N]T [N]C [N]A [N]C [N]T [N]G [N]C [N]T [N]G [N]G [N]C [N]C [N]A [N]C [N]A [N]G [N]A [N]A [N]C [N]C [N]A [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]T [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]T [N]G [N]C [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]A [N]A [N]T [N]C [N]A [N]A [N]T\n",
      "CHUNK TOKENS: ['[N]A', '[N]C', '[N]T', '[N]A', '[N]G', '[N]G', '[N]T', '[N]G', '[N]G', '[N]G', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]T', '[N]G', '[N]C', '[N]A', '[N]T', '[N]A', '[N]G', '[N]C', '[N]T', '[N]G', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]C', '[N]A', '[N]G', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]A', '[N]G', '[N]G', '[N]T', '[N]T', '[N]C', '[N]T', '[N]T', '[N]T', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]G', '[N]G', '[N]A', '[N]C', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]A', '[N]G', '[N]G', '[N]G', '[N]G', '[N]T', '[N]G', '[N]T', '[N]T', '[N]A', '[N]C', '[N]C', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]T', '[N]A', '[N]A', '[N]A', '[N]C', '[N]C', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]G', '[N]A', '[N]C', '[N]A', '[N]G', '[N]T', '[N]A', '[N]T', '[N]G', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]A', '[N]G', '[N]A', '[N]G', '[N]C', '[N]A', '[N]T', '[N]A', '[N]T', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]G', '[N]G', '[N]C', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]A', '[N]A', '[N]C', '[N]C', '[N]A', '[N]C', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]C', '[N]A', '[N]A', '[N]T', '[N]C', '[N]A', '[N]A', '[N]T']\n",
      "COUNTER: 2\n",
      "LEFTOVER:  [N]\n",
      "CHUNK: C [N]T [N]G [N]G [N]T [N]T [N]T [N]C [N]T [N]T [N]A [N]C [N]A [N]G [N]G [N]C [N]C [N]C [N]T [N]T [N]G [N]A [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]C [N]T [N]G [N]T [N]T [N]T [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]G [N]C [N]T [N]T [N]T [N]T [N]G [N]C [N]A [N]A [N]A [N]T [N]C [N]A [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]T [N]A [N]T [N]A [N]C [N]A [N]G [N]G [N]T [N]A [N]A [N]T [N]T [N]T [N]G [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]G [N]T [N]C [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]A [N]C [N]A [N]T [N]T [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]A [N]T [N]A [N]T [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]G [N]C [N]T [N]A [N]A [N]A [N]A [N]T [N]T [N]T [N]T [N]A [N]C [N]G [N]G [N]C [N]T [N]A [N]T [N]A [N]C [N]T [N]C [N]C [N]T [N]T [N]G [N]T [N]G [N]A [N]C [N]A [N]G [N]C [N]A [N]G [N]A [N]T [N]C [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]G [N]G [N]G [N]G [N]A [N]A [N]G [N]G [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]A [N]G [N]G [N]C [N\n",
      "LAST SEP: 1021\n",
      "TOKEN CHUNK:  [N]C [N]T [N]G [N]G [N]T [N]T [N]T [N]C [N]T [N]T [N]A [N]C [N]A [N]G [N]G [N]C [N]C [N]C [N]T [N]T [N]G [N]A [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]C [N]T [N]G [N]T [N]T [N]T [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]G [N]C [N]T [N]T [N]T [N]T [N]G [N]C [N]A [N]A [N]A [N]T [N]C [N]A [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]T [N]A [N]T [N]A [N]C [N]A [N]G [N]G [N]T [N]A [N]A [N]T [N]T [N]T [N]G [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]G [N]T [N]C [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]A [N]C [N]A [N]T [N]T [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]A [N]T [N]A [N]T [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]G [N]C [N]T [N]A [N]A [N]A [N]A [N]T [N]T [N]T [N]T [N]A [N]C [N]G [N]G [N]C [N]T [N]A [N]T [N]A [N]C [N]T [N]C [N]C [N]T [N]T [N]G [N]T [N]G [N]A [N]C [N]A [N]G [N]C [N]A [N]G [N]A [N]T [N]C [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]G [N]G [N]G [N]G [N]A [N]A [N]G [N]G [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]A [N]G [N]G [N]C\n",
      "CHUNK TOKENS: ['[N]C', '[N]T', '[N]G', '[N]G', '[N]T', '[N]T', '[N]T', '[N]C', '[N]T', '[N]T', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]C', '[N]C', '[N]C', '[N]T', '[N]T', '[N]G', '[N]A', '[N]A', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]C', '[N]T', '[N]G', '[N]T', '[N]T', '[N]T', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]C', '[N]G', '[N]C', '[N]T', '[N]T', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]G', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]G', '[N]T', '[N]G', '[N]T', '[N]C', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]A', '[N]C', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]C', '[N]C', '[N]T', '[N]A', '[N]C', '[N]A', '[N]T', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]C', '[N]T', '[N]A', '[N]A', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]A', '[N]C', '[N]G', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]T', '[N]T', '[N]G', '[N]T', '[N]G', '[N]A', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]A', '[N]T', '[N]C', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]G', '[N]G', '[N]G', '[N]G', '[N]A', '[N]A', '[N]G', '[N]G', '[N]A', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]G', '[N]T', '[N]A', '[N]G', '[N]G', '[N]C']\n",
      "COUNTER: 3\n",
      "LEFTOVER:  [N\n",
      "CHUNK: ]A [N]C [N]T [N]G [N]T [N]G [N]G [N]C [N]A [N]G [N]G [N]C [N]T [N]T [N]G [N]C [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]G [N]G [N]C [N]G [N]G [N]G [N]A [N]A [N]T [N]G [N]A [N]A [N]A [N]T [N]G [N]A [N]A [N]G [N]T [N]A [N]C [N]A [N]G [N]T [N]G [N]T [N]G [N]G [N]T [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]A [N]C [N]A [N]C [N]T [N]G [N]A [N]A [N]G [N]T [N]C [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]T [N]G [N]A [N]G [N]C [N]T [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]G [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]T [N]T [N]C [N]C [N]C [N]C [N]A [N]T [N]C [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]T [N]G [N]A [N]T [N]T [N]A [N]T [N]A [N]A [N]T [N]A [N]A [N]C [N]T [N]G [N]C [N]A [N]C [N]T [N]T [N]A [N]C [N]T [N]T [N]C [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]A [N]C [N]T [N]G [N]G [N]G [N]A [N]C [N]A [\n",
      "LAST SEP: 1022\n",
      "TOKEN CHUNK:  [N]A [N]C [N]T [N]G [N]T [N]G [N]G [N]C [N]A [N]G [N]G [N]C [N]T [N]T [N]G [N]C [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]G [N]G [N]C [N]G [N]G [N]G [N]A [N]A [N]T [N]G [N]A [N]A [N]A [N]T [N]G [N]A [N]A [N]G [N]T [N]A [N]C [N]A [N]G [N]T [N]G [N]T [N]G [N]G [N]T [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]A [N]C [N]A [N]C [N]T [N]G [N]A [N]A [N]G [N]T [N]C [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]T [N]G [N]A [N]G [N]C [N]T [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]G [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]T [N]T [N]C [N]C [N]C [N]C [N]A [N]T [N]C [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]T [N]G [N]A [N]T [N]T [N]A [N]T [N]A [N]A [N]T [N]A [N]A [N]C [N]T [N]G [N]C [N]A [N]C [N]T [N]T [N]A [N]C [N]T [N]T [N]C [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]A [N]C [N]T [N]G [N]G [N]G [N]A [N]C [N]A\n",
      "CHUNK TOKENS: ['[N]A', '[N]C', '[N]T', '[N]G', '[N]T', '[N]G', '[N]G', '[N]C', '[N]A', '[N]G', '[N]G', '[N]C', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]A', '[N]T', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]T', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]T', '[N]T', '[N]A', '[N]G', '[N]A', '[N]G', '[N]G', '[N]C', '[N]G', '[N]G', '[N]G', '[N]A', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]T', '[N]A', '[N]C', '[N]A', '[N]G', '[N]T', '[N]G', '[N]T', '[N]G', '[N]G', '[N]T', '[N]G', '[N]G', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]A', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]T', '[N]C', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]G', '[N]T', '[N]G', '[N]A', '[N]G', '[N]C', '[N]T', '[N]T', '[N]A', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]G', '[N]T', '[N]T', '[N]A', '[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]G', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]G', '[N]T', '[N]T', '[N]T', '[N]C', '[N]C', '[N]C', '[N]C', '[N]A', '[N]T', '[N]C', '[N]T', '[N]G', '[N]T', '[N]A', '[N]A', '[N]T', '[N]G', '[N]T', '[N]G', '[N]A', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]A', '[N]T', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]A', '[N]C', '[N]T', '[N]T', '[N]A', '[N]C', '[N]T', '[N]T', '[N]C', '[N]A', '[N]T', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]G', '[N]A', '[N]T', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]A', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]A', '[N]C', '[N]A']\n",
      "COUNTER: 4\n",
      "LEFTOVER:  [\n",
      "CHUNK: N]T [N]T [N]T [N]A [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]A [N]A [N]A [N]T [N]G [N]C [N]A [N]T [N]G [N]T [N]A [N]A [N]G [N]T [N]G [N]C [N]T [N]C [N]A [N]T [N]C [N]C [N]T [N]A [N]A [N]T [N]A [N]C [N]A [N]A [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]T [N]T [N]T [N]T [N]T [N]A [N]A [N]A [N]C [N]A [N]G [N]C [N]C [N]C [N]T [N]G [N]C [N]C [N]G [N]G [N]T [N]T [N]A [N]G [N]G [N]C [N]A [N]T [N]T [N]A [N]C [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]A [N]T [N]G [N]A [N]G [N]G [N]C [N]A [N]G [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]A [N]C [N]A [N]G [N]G [N]T [N]G [N]G [N]T [N]A [N]C [N]C [N]A [N]G [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]A [N]T [N]C [N]C [N]A [N]G [N]A [N]A [N]C [N]T [N]G [N]C [N]C [N]C [N]A [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]G [N]C [N]T [N]C [N]A [N]A [N]A [N]C [N]T [N]C [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]C [N]A [N]C [N]C [N]T [N]G [N]C [N]C [N]A [N]G [N]C [N]T [N]G \n",
      "LAST SEP: 1023\n",
      "TOKEN CHUNK:  [N]T [N]T [N]T [N]A [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]A [N]A [N]A [N]T [N]G [N]C [N]A [N]T [N]G [N]T [N]A [N]A [N]G [N]T [N]G [N]C [N]T [N]C [N]A [N]T [N]C [N]C [N]T [N]A [N]A [N]T [N]A [N]C [N]A [N]A [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]T [N]T [N]T [N]T [N]T [N]A [N]A [N]A [N]C [N]A [N]G [N]C [N]C [N]C [N]T [N]G [N]C [N]C [N]G [N]G [N]T [N]T [N]A [N]G [N]G [N]C [N]A [N]T [N]T [N]A [N]C [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]A [N]T [N]G [N]A [N]G [N]G [N]C [N]A [N]G [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]A [N]C [N]A [N]G [N]G [N]T [N]G [N]G [N]T [N]A [N]C [N]C [N]A [N]G [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]A [N]T [N]C [N]C [N]A [N]G [N]A [N]A [N]C [N]T [N]G [N]C [N]C [N]C [N]A [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]G [N]C [N]T [N]C [N]A [N]A [N]A [N]C [N]T [N]C [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]C [N]A [N]C [N]C [N]T [N]G [N]C [N]C [N]A [N]G [N]C [N]T [N]G\n",
      "CHUNK TOKENS: ['[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]A', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]C', '[N]A', '[N]T', '[N]G', '[N]T', '[N]A', '[N]A', '[N]G', '[N]T', '[N]G', '[N]C', '[N]T', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]A', '[N]A', '[N]T', '[N]A', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]T', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]G', '[N]T', '[N]T', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]T', '[N]A', '[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]A', '[N]C', '[N]A', '[N]G', '[N]C', '[N]C', '[N]C', '[N]T', '[N]G', '[N]C', '[N]C', '[N]G', '[N]G', '[N]T', '[N]T', '[N]A', '[N]G', '[N]G', '[N]C', '[N]A', '[N]T', '[N]T', '[N]A', '[N]C', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]A', '[N]T', '[N]G', '[N]A', '[N]G', '[N]G', '[N]C', '[N]A', '[N]G', '[N]T', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]G', '[N]G', '[N]T', '[N]A', '[N]C', '[N]C', '[N]A', '[N]G', '[N]G', '[N]A', '[N]T', '[N]T', '[N]A', '[N]G', '[N]A', '[N]A', '[N]T', '[N]C', '[N]C', '[N]A', '[N]G', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]C', '[N]C', '[N]A', '[N]A', '[N]C', '[N]C', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]G', '[N]C', '[N]T', '[N]C', '[N]A', '[N]A', '[N]A', '[N]C', '[N]T', '[N]C', '[N]T', '[N]T', '[N]C', '[N]C', '[N]T', '[N]A', '[N]C', '[N]C', '[N]A', '[N]C', '[N]C', '[N]T', '[N]G', '[N]C', '[N]C', '[N]A', '[N]G', '[N]C', '[N]T', '[N]G']\n",
      "COUNTER: 5\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "test_tokens = []\n",
    "\n",
    "\n",
    "with all_annotation_file_paths[0].open('r') as f:\n",
    "    leftover_text = \"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        print(f\"COUNTER: {counter}\")\n",
    "        token_string = f.read(1024)\n",
    "        if token_string == \"\" or counter == 5:\n",
    "            break\n",
    "        print(f\"LEFTOVER: {leftover_text}\")\n",
    "        print(f\"CHUNK: {token_string}\")\n",
    "        last_sep = token_string.rindex(\" \")\n",
    "        print(f\"LAST SEP: {last_sep}\")\n",
    "        if last_sep != -1:\n",
    "            # Get our chunk of uninterrupted tokens\n",
    "            token_chunk = leftover_text + token_string[:last_sep]\n",
    "            print(f\"TOKEN CHUNK: {token_chunk}\")\n",
    "            # Get our list of tokenized tokens\n",
    "            chunk_tokens = [tok for tok in token_chunk.split(\" \") if len(tok) > 0]\n",
    "            print(f\"CHUNK TOKENS: {chunk_tokens}\")\n",
    "            # Record the leftover string not in the token string\n",
    "            leftover_text = token_string[last_sep:]\n",
    "            # yield our list of tokens\n",
    "            test_tokens.append(chunk_tokens)\n",
    "        else:\n",
    "            leftover_text = leftover_text + token_string\n",
    "        counter += 1\n",
    "    test_tokens.append(leftover_text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f4190-77c0-4c70-bb15-91594ffc5982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000003.12/100129480/gene.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_annotation_file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531807d-ec80-45c6-a322-9251fb97d084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21307"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(tok) for tok in test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53ef94-1ce6-4003-ba1b-7dc794bccf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def yield_file_tokens(path: Path, chunksize=1024, sep=\" \") -> list[str]:\n",
    "    with path.open('r') as f:\n",
    "        leftover_text = \"\"\n",
    "        while True:\n",
    "            token_string = f.read(chunksize)\n",
    "            if token_string == \"\":\n",
    "                break\n",
    "            try:\n",
    "                last_sep = token_string.rindex(\" \")\n",
    "                # Get our chunk of uninterrupted tokens\n",
    "                token_chunk = leftover_text + token_string[:last_sep]\n",
    "                # Get our list of tokenized tokens\n",
    "                chunk_tokens = [tok for tok in token_chunk.split(\" \") if len(tok) > 0]\n",
    "                # Record the leftover string not in the token string\n",
    "                leftover_text = token_string[last_sep:]\n",
    "                # yield our list of tokens\n",
    "                yield chunk_tokens\n",
    "            except ValueError:\n",
    "                # If there is no sep, capture the string\n",
    "                leftover_text = leftover_text + token_string\n",
    "        chunk_tokens = [tok for tok in leftover_text.split(\" \") if len(tok) > 0]\n",
    "        if len(chunk_tokens) > 0:\n",
    "            yield chunk_tokens\n",
    "\n",
    "\n",
    "def yield_all_training_tokens(file_paths: list[Path]):\n",
    "    file_path_tqdm = tqdm(file_paths)\n",
    "    return map(lambda path: yield_file_tokens(path), file_path_tqdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e08ff-7b7d-4517-8145-dc018787d4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21224"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "test_file_tokens = list(yield_file_tokens(all_annotation_file_paths[0]))\n",
    "sum([len(token_list) for token_list in test_file_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44338d93-a70c-4365-9481-ed7da264b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torchtext.vocab import Vocab, vocab\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "def build_vocab(file_paths: list[Path], special_tokens: list[str] = ['<unk>']) -> Vocab:\n",
    "    # Count tokens in each file\n",
    "    token_counter = Counter()\n",
    "    try:\n",
    "        for path in tqdm(file_paths):\n",
    "            for tokens in yield_file_tokens(path):\n",
    "                token_counter.update(tokens)\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    print(token_counter)\n",
    "    ordered_counter = OrderedDict(token_counter.most_common())\n",
    "    token_vocab = vocab(ordered_counter, specials=special_tokens)\n",
    "    return token_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350a7a0-6382-4b00-8354-c78067c3b79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3348a5af819421b8acfc82467997aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'[N]T': 63851, '[N]A': 54272, '[N]G': 40072, '[N]C': 38971})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "test_vocab = build_vocab(all_annotation_file_paths[:4])\n",
    "test_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943f1c7-fdd5-4c90-9fb0-84f60349cc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "test_vocab[\"[N]T\"], test_vocab[\"[N]A\"], test_vocab[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd32cc-6879-47e6-b5b2-64bdd4e79a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Save the vocabulary\n",
    "artefacts_path = data_path / \"artefacts\"\n",
    "training_artefacts_path = artefacts_path / \"training\"\n",
    "if not training_artefacts_path.exists():\n",
    "    training_artefacts_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14992ac9-884b-4740-a763-c145e4c843e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90eee54531434e8bb9f56225b1c7d807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "import torch\n",
    "\n",
    "vocab_path = training_artefacts_path / \"vocab.pt\"\n",
    "if not vocab_path.exists():\n",
    "    vocab = build_vocab(all_annotation_file_paths)\n",
    "    torch.save(vocab, vocab_path)\n",
    "else:\n",
    "    vocab = torch.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e06c7e-a0bd-4553-b26d-e94f6f936b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "vocab[\"[N]T\"], vocab[\"[N]A\"], vocab[\"[N]C\"], vocab[\"[N]G\"], vocab[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a5f46-1270-4753-9fef-7e893963d1f3",
   "metadata": {},
   "source": [
    "### Build our training, validation, test idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c85be0-f3c4-42db-b40e-895a78cbf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Build indices for train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898ad26-f536-4259-987b-37406da51d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9946d-8338-4b74-bc99-d18050b00447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc009b-0f0a-4364-9530-68a2784f8c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5113515-92c2-4825-b7f6-5f972384d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
