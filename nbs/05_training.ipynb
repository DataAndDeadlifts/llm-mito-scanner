{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4937e64-3ce6-4ec8-9d8e-7f05e9ba3dba",
   "metadata": {},
   "source": [
    "# Training our foundational model\n",
    "\n",
    "> \"Lets start training!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e4705-2246-4933-bace-9cdf64adeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.foundation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6ab3e-2354-4aef-b44f-e9c6ae451e8a",
   "metadata": {},
   "source": [
    "## Setup, indexing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8df81-dadc-4661-ae90-47e31f0de989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages/Bio/__init__.py:138: BiopythonWarning: You may be importing Biopython from inside the source tree. This is bad practice and might lead to downstream issues. In particular, you might encounter ImportErrors due to missing compiled C extensions. We recommend that you try running your code from outside the source tree. If you are outside the source tree then you have a setup.py file in an unexpected directory: /home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from llm_mito_scanner.analysis.training import get_training_annotation_paths\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a50d407-6de9-4d35-97e4-ec94a635ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from yaml import safe_load\n",
    "\n",
    "tqdm.pandas(ncols=80, leave=False)\n",
    "\n",
    "with open(\"../config.yml\") as f:\n",
    "    config = safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22252b2-60fc-43d2-a616-a7e1df49fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "data_path = Path(config.get(\"data_path\"))\n",
    "training_data_path = data_path / \"training\"\n",
    "training_index_path = data_path / \"training_index.csv\"\n",
    "gene_to_protein_maps_path = data_path / \"gene_to_protein_maps.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99df0cf8-326f-4979-b109-b2424f4051a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import json\n",
    "\n",
    "if not training_index_path.exists() or not gene_to_protein_maps_path.exists():\n",
    "    gene_to_protein_maps, training_paths = get_training_annotation_paths(training_data_path)\n",
    "    training_paths.to_csv(training_index_path, index=False)\n",
    "    gene_to_protein_maps.to_csv(gene_to_protein_maps_path)\n",
    "else:\n",
    "    training_paths = pd.read_csv(training_index_path)\n",
    "    gene_to_protein_maps = pd.read_csv(gene_to_protein_maps_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b16ce-590a-4d3c-8c27-77f891a52dd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>gene</th>\n",
       "      <th>gene_annotation</th>\n",
       "      <th>protein_annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NC_000003</td>\n",
       "      <td>100129480</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/trainin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotation       gene                                    gene_annotation   \n",
       "0  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \\\n",
       "1  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "2  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "3  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "4  NC_000003  100129480  /mnt/e/Data/llm-mito-scanner-data/data/trainin...   \n",
       "\n",
       "                                  protein_annotation  \n",
       "0  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "1  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "2  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "3  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  \n",
       "4  /mnt/e/Data/llm-mito-scanner-data/data/trainin...  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "training_paths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f2c355-4ffc-4134-aeb5-74a29207f92b",
   "metadata": {},
   "source": [
    "## Build the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9529859-4c8a-4ddd-a14b-62f71ea979de",
   "metadata": {},
   "source": [
    "### Construct the tokenizer, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93626513-9c6d-4c2b-9ae4-9104f0c64031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from fastai.text.core import BaseTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acba19f8-b4b7-4dcd-b702-d13b86fd8d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "tokenizer = BaseTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea5e8e-7eb9-44e3-9208-8147b3ee5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "gene_file_paths = [Path(p) for p in training_paths.gene_annotation.dropna().unique().tolist()]\n",
    "protein_file_paths = [Path(p) for p in training_paths.protein_annotation.dropna().unique().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be241a-332c-4b26-8436-797843df5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "all_annotation_file_paths = gene_file_paths + protein_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc1b0b-f9e6-4b23-bba7-3a106c305c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTER: 0\n",
      "LEFTOVER: \n",
      "CHUNK: [N]A [N]C [N]A [N]T [N]C [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]T [N]C [N]C [N]T [N]T [N]T [N]G [N]G [N]G [N]G [N]C [N]A [N]T [N]C [N]T [N]C [N]T [N]G [N]T [N]C [N]A [N]T [N]G [N]T [N]G [N]C [N]T [N]T [N]A [N]T [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]C [N]C [N]T [N]C [N]T [N]C [N]C [N]A [N]T [N]C [N]T [N]A [N]T [N]G [N]T [N]T [N]A [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]C [N]T [N]T [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]G [N]C [N]C [N]T [N]C [N]T [N]T [N]T [N]C [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]G [N]C [N]T [N]T [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]A [N]A [N]T [N]T [N]T [N]C [N]C [N]A [N]A [N]C [N]T [N]G [N]C [N]T [N]C [N]A [N]A [N]C [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]A [N]T [N]G [N]G [N]A [N]C [N]A [N]A [N]A [N]C [N]C [N]G [N]C [N]C [N]C [N]C [N]T [N]C [N]A [N]T [N]A [N]T [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]C [N]A [N]T [N]G [N]A [N]A [N]C [N]T [N]G [N]G [N]A [N]G [N]G [N]A [N]G [N]G [N]G [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G [N]A\n",
      "LAST SEP: 1019\n",
      "TOKEN CHUNK: [N]A [N]C [N]A [N]T [N]C [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]T [N]C [N]C [N]T [N]T [N]T [N]G [N]G [N]G [N]G [N]C [N]A [N]T [N]C [N]T [N]C [N]T [N]G [N]T [N]C [N]A [N]T [N]G [N]T [N]G [N]C [N]T [N]T [N]A [N]T [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]C [N]C [N]T [N]C [N]T [N]C [N]C [N]A [N]T [N]C [N]T [N]A [N]T [N]G [N]T [N]T [N]A [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]C [N]T [N]T [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]G [N]C [N]C [N]T [N]C [N]T [N]T [N]T [N]C [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]G [N]C [N]T [N]T [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]A [N]A [N]T [N]T [N]T [N]C [N]C [N]A [N]A [N]C [N]T [N]G [N]C [N]T [N]C [N]A [N]A [N]C [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]A [N]T [N]G [N]G [N]A [N]C [N]A [N]A [N]A [N]C [N]C [N]G [N]C [N]C [N]C [N]C [N]T [N]C [N]A [N]T [N]A [N]T [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]C [N]A [N]T [N]G [N]A [N]A [N]C [N]T [N]G [N]G [N]A [N]G [N]G [N]A [N]G [N]G [N]G [N]G [N]A [N]G [N]A [N]G [N]A [N]C [N]G\n",
      "CHUNK TOKENS: ['[N]A', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]T', '[N]G', '[N]T', '[N]C', '[N]C', '[N]T', '[N]T', '[N]T', '[N]G', '[N]G', '[N]G', '[N]G', '[N]C', '[N]A', '[N]T', '[N]C', '[N]T', '[N]C', '[N]T', '[N]G', '[N]T', '[N]C', '[N]A', '[N]T', '[N]G', '[N]T', '[N]G', '[N]C', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]G', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]T', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]T', '[N]C', '[N]T', '[N]A', '[N]T', '[N]G', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]T', '[N]C', '[N]T', '[N]T', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]G', '[N]C', '[N]C', '[N]T', '[N]C', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]G', '[N]C', '[N]T', '[N]T', '[N]T', '[N]G', '[N]T', '[N]A', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]C', '[N]A', '[N]A', '[N]C', '[N]C', '[N]T', '[N]T', '[N]T', '[N]C', '[N]T', '[N]G', '[N]A', '[N]T', '[N]G', '[N]G', '[N]A', '[N]C', '[N]A', '[N]A', '[N]A', '[N]C', '[N]C', '[N]G', '[N]C', '[N]C', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]T', '[N]A', '[N]T', '[N]C', '[N]T', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]G', '[N]A', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]G', '[N]A', '[N]G', '[N]G', '[N]A', '[N]G', '[N]G', '[N]G', '[N]G', '[N]A', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]G']\n",
      "COUNTER: 1\n",
      "LEFTOVER:  [N]A\n",
      "CHUNK:  [N]C [N]T [N]A [N]G [N]G [N]T [N]G [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]T [N]G [N]C [N]A [N]T [N]A [N]G [N]C [N]T [N]G [N]G [N]A [N]G [N]A [N]C [N]T [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]G [N]G [N]T [N]A [N]G [N]G [N]T [N]T [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]G [N]G [N]G [N]G [N]A [N]C [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]A [N]G [N]G [N]G [N]G [N]T [N]G [N]T [N]T [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]T [N]A [N]A [N]A [N]C [N]C [N]C [N]A [N]A [N]A [N]A [N]G [N]A [N]C [N]T [N]G [N]T [N]T [N]T [N]C [N]A [N]G [N]A [N]C [N]A [N]G [N]T [N]A [N]T [N]G [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]A [N]C [N]A [N]C [N]T [N]G [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]T [N]A [N]T [N]C [N]A [N]G [N]G [N]T [N]T [N]C [N]A [N]C [N]T [N]G [N]C [N]T [N]G [N]G [N]C [N]C [N]A [N]C [N]A [N]G [N]A [N]A [N]C [N]C [N]A [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]T [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]T [N]G [N]C [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]A [N]A [N]T [N]C [N]A [N]A [N]T [N]\n",
      "LAST SEP: 1020\n",
      "TOKEN CHUNK:  [N]A [N]C [N]T [N]A [N]G [N]G [N]T [N]G [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]T [N]G [N]C [N]A [N]T [N]A [N]G [N]C [N]T [N]G [N]G [N]A [N]G [N]A [N]C [N]T [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]G [N]G [N]T [N]A [N]G [N]G [N]T [N]T [N]C [N]T [N]T [N]T [N]C [N]T [N]G [N]G [N]G [N]G [N]G [N]A [N]C [N]A [N]C [N]T [N]C [N]C [N]A [N]A [N]A [N]G [N]G [N]G [N]G [N]T [N]G [N]T [N]T [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]T [N]A [N]A [N]A [N]C [N]C [N]C [N]A [N]A [N]A [N]A [N]G [N]A [N]C [N]T [N]G [N]T [N]T [N]T [N]C [N]A [N]G [N]A [N]C [N]A [N]G [N]T [N]A [N]T [N]G [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]A [N]C [N]A [N]C [N]T [N]G [N]C [N]A [N]G [N]A [N]G [N]C [N]A [N]T [N]A [N]T [N]C [N]A [N]G [N]G [N]T [N]T [N]C [N]A [N]C [N]T [N]G [N]C [N]T [N]G [N]G [N]C [N]C [N]A [N]C [N]A [N]G [N]A [N]A [N]C [N]C [N]A [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]T [N]C [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]C [N]A [N]G [N]T [N]G [N]C [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]A [N]A [N]T [N]C [N]A [N]A [N]T\n",
      "CHUNK TOKENS: ['[N]A', '[N]C', '[N]T', '[N]A', '[N]G', '[N]G', '[N]T', '[N]G', '[N]G', '[N]G', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]T', '[N]G', '[N]C', '[N]A', '[N]T', '[N]A', '[N]G', '[N]C', '[N]T', '[N]G', '[N]G', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]C', '[N]A', '[N]G', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]A', '[N]G', '[N]G', '[N]T', '[N]T', '[N]C', '[N]T', '[N]T', '[N]T', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]G', '[N]G', '[N]A', '[N]C', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]A', '[N]G', '[N]G', '[N]G', '[N]G', '[N]T', '[N]G', '[N]T', '[N]T', '[N]A', '[N]C', '[N]C', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]T', '[N]A', '[N]A', '[N]A', '[N]C', '[N]C', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]G', '[N]A', '[N]C', '[N]A', '[N]G', '[N]T', '[N]A', '[N]T', '[N]G', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]A', '[N]G', '[N]A', '[N]G', '[N]C', '[N]A', '[N]T', '[N]A', '[N]T', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]G', '[N]G', '[N]C', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]A', '[N]A', '[N]C', '[N]C', '[N]A', '[N]C', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]C', '[N]T', '[N]C', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]C', '[N]A', '[N]A', '[N]T', '[N]C', '[N]A', '[N]A', '[N]T']\n",
      "COUNTER: 2\n",
      "LEFTOVER:  [N]\n",
      "CHUNK: C [N]T [N]G [N]G [N]T [N]T [N]T [N]C [N]T [N]T [N]A [N]C [N]A [N]G [N]G [N]C [N]C [N]C [N]T [N]T [N]G [N]A [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]C [N]T [N]G [N]T [N]T [N]T [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]G [N]C [N]T [N]T [N]T [N]T [N]G [N]C [N]A [N]A [N]A [N]T [N]C [N]A [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]T [N]A [N]T [N]A [N]C [N]A [N]G [N]G [N]T [N]A [N]A [N]T [N]T [N]T [N]G [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]G [N]T [N]C [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]A [N]C [N]A [N]T [N]T [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]A [N]T [N]A [N]T [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]G [N]C [N]T [N]A [N]A [N]A [N]A [N]T [N]T [N]T [N]T [N]A [N]C [N]G [N]G [N]C [N]T [N]A [N]T [N]A [N]C [N]T [N]C [N]C [N]T [N]T [N]G [N]T [N]G [N]A [N]C [N]A [N]G [N]C [N]A [N]G [N]A [N]T [N]C [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]G [N]G [N]G [N]G [N]A [N]A [N]G [N]G [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]A [N]G [N]G [N]C [N\n",
      "LAST SEP: 1021\n",
      "TOKEN CHUNK:  [N]C [N]T [N]G [N]G [N]T [N]T [N]T [N]C [N]T [N]T [N]A [N]C [N]A [N]G [N]G [N]C [N]C [N]C [N]T [N]T [N]G [N]A [N]A [N]T [N]G [N]T [N]T [N]G [N]C [N]C [N]T [N]G [N]T [N]T [N]T [N]A [N]G [N]C [N]A [N]A [N]A [N]A [N]C [N]G [N]C [N]T [N]T [N]T [N]T [N]G [N]C [N]A [N]A [N]A [N]T [N]C [N]A [N]C [N]T [N]T [N]C [N]C [N]A [N]A [N]G [N]T [N]A [N]T [N]A [N]C [N]A [N]G [N]G [N]T [N]A [N]A [N]T [N]T [N]T [N]G [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]G [N]T [N]C [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]T [N]A [N]C [N]A [N]T [N]T [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]A [N]T [N]A [N]T [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]G [N]C [N]T [N]A [N]A [N]A [N]A [N]T [N]T [N]T [N]T [N]A [N]C [N]G [N]G [N]C [N]T [N]A [N]T [N]A [N]C [N]T [N]C [N]C [N]T [N]T [N]G [N]T [N]G [N]A [N]C [N]A [N]G [N]C [N]A [N]G [N]A [N]T [N]C [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]G [N]G [N]G [N]G [N]A [N]A [N]G [N]G [N]A [N]A [N]G [N]C [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]A [N]G [N]G [N]C\n",
      "CHUNK TOKENS: ['[N]C', '[N]T', '[N]G', '[N]G', '[N]T', '[N]T', '[N]T', '[N]C', '[N]T', '[N]T', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]C', '[N]C', '[N]C', '[N]T', '[N]T', '[N]G', '[N]A', '[N]A', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]C', '[N]T', '[N]G', '[N]T', '[N]T', '[N]T', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]C', '[N]G', '[N]C', '[N]T', '[N]T', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]T', '[N]C', '[N]C', '[N]A', '[N]A', '[N]G', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]G', '[N]T', '[N]G', '[N]T', '[N]C', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]A', '[N]C', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]C', '[N]C', '[N]T', '[N]A', '[N]C', '[N]A', '[N]T', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]C', '[N]T', '[N]A', '[N]A', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]A', '[N]C', '[N]G', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]A', '[N]C', '[N]T', '[N]C', '[N]C', '[N]T', '[N]T', '[N]G', '[N]T', '[N]G', '[N]A', '[N]C', '[N]A', '[N]G', '[N]C', '[N]A', '[N]G', '[N]A', '[N]T', '[N]C', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]G', '[N]G', '[N]G', '[N]G', '[N]A', '[N]A', '[N]G', '[N]G', '[N]A', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]G', '[N]T', '[N]A', '[N]G', '[N]G', '[N]C']\n",
      "COUNTER: 3\n",
      "LEFTOVER:  [N\n",
      "CHUNK: ]A [N]C [N]T [N]G [N]T [N]G [N]G [N]C [N]A [N]G [N]G [N]C [N]T [N]T [N]G [N]C [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]G [N]G [N]C [N]G [N]G [N]G [N]A [N]A [N]T [N]G [N]A [N]A [N]A [N]T [N]G [N]A [N]A [N]G [N]T [N]A [N]C [N]A [N]G [N]T [N]G [N]T [N]G [N]G [N]T [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]A [N]C [N]A [N]C [N]T [N]G [N]A [N]A [N]G [N]T [N]C [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]T [N]G [N]A [N]G [N]C [N]T [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]G [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]T [N]T [N]C [N]C [N]C [N]C [N]A [N]T [N]C [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]T [N]G [N]A [N]T [N]T [N]A [N]T [N]A [N]A [N]T [N]A [N]A [N]C [N]T [N]G [N]C [N]A [N]C [N]T [N]T [N]A [N]C [N]T [N]T [N]C [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]A [N]C [N]T [N]G [N]G [N]G [N]A [N]C [N]A [\n",
      "LAST SEP: 1022\n",
      "TOKEN CHUNK:  [N]A [N]C [N]T [N]G [N]T [N]G [N]G [N]C [N]A [N]G [N]G [N]C [N]T [N]T [N]G [N]C [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]T [N]A [N]C [N]T [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]G [N]G [N]C [N]G [N]G [N]G [N]A [N]A [N]T [N]G [N]A [N]A [N]A [N]T [N]G [N]A [N]A [N]G [N]T [N]A [N]C [N]A [N]G [N]T [N]G [N]T [N]G [N]G [N]T [N]G [N]G [N]T [N]G [N]A [N]A [N]G [N]A [N]C [N]A [N]C [N]T [N]G [N]A [N]A [N]G [N]T [N]C [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]T [N]G [N]A [N]G [N]C [N]T [N]T [N]A [N]A [N]G [N]C [N]A [N]A [N]G [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]G [N]T [N]C [N]A [N]C [N]T [N]G [N]A [N]G [N]C [N]C [N]T [N]C [N]A [N]G [N]T [N]T [N]T [N]C [N]C [N]C [N]C [N]A [N]T [N]C [N]T [N]G [N]T [N]A [N]A [N]T [N]G [N]T [N]G [N]A [N]T [N]T [N]A [N]T [N]A [N]A [N]T [N]A [N]A [N]C [N]T [N]G [N]C [N]A [N]C [N]T [N]T [N]A [N]C [N]T [N]T [N]C [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]G [N]C [N]T [N]A [N]T [N]G [N]A [N]T [N]A [N]G [N]A [N]C [N]T [N]A [N]C [N]T [N]G [N]G [N]G [N]A [N]C [N]A\n",
      "CHUNK TOKENS: ['[N]A', '[N]C', '[N]T', '[N]G', '[N]T', '[N]G', '[N]G', '[N]C', '[N]A', '[N]G', '[N]G', '[N]C', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]A', '[N]T', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]T', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]T', '[N]T', '[N]A', '[N]G', '[N]A', '[N]G', '[N]G', '[N]C', '[N]G', '[N]G', '[N]G', '[N]A', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]T', '[N]A', '[N]C', '[N]A', '[N]G', '[N]T', '[N]G', '[N]T', '[N]G', '[N]G', '[N]T', '[N]G', '[N]G', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]A', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]A', '[N]G', '[N]T', '[N]C', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]G', '[N]T', '[N]G', '[N]A', '[N]G', '[N]C', '[N]T', '[N]T', '[N]A', '[N]A', '[N]G', '[N]C', '[N]A', '[N]A', '[N]G', '[N]T', '[N]T', '[N]A', '[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]G', '[N]T', '[N]C', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]G', '[N]T', '[N]T', '[N]T', '[N]C', '[N]C', '[N]C', '[N]C', '[N]A', '[N]T', '[N]C', '[N]T', '[N]G', '[N]T', '[N]A', '[N]A', '[N]T', '[N]G', '[N]T', '[N]G', '[N]A', '[N]T', '[N]T', '[N]A', '[N]T', '[N]A', '[N]A', '[N]T', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]A', '[N]C', '[N]T', '[N]T', '[N]A', '[N]C', '[N]T', '[N]T', '[N]C', '[N]A', '[N]T', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]G', '[N]A', '[N]T', '[N]A', '[N]G', '[N]A', '[N]C', '[N]T', '[N]A', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]A', '[N]C', '[N]A']\n",
      "COUNTER: 4\n",
      "LEFTOVER:  [\n",
      "CHUNK: N]T [N]T [N]T [N]A [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]A [N]A [N]A [N]T [N]G [N]C [N]A [N]T [N]G [N]T [N]A [N]A [N]G [N]T [N]G [N]C [N]T [N]C [N]A [N]T [N]C [N]C [N]T [N]A [N]A [N]T [N]A [N]C [N]A [N]A [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]T [N]T [N]T [N]T [N]T [N]A [N]A [N]A [N]C [N]A [N]G [N]C [N]C [N]C [N]T [N]G [N]C [N]C [N]G [N]G [N]T [N]T [N]A [N]G [N]G [N]C [N]A [N]T [N]T [N]A [N]C [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]A [N]T [N]G [N]A [N]G [N]G [N]C [N]A [N]G [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]A [N]C [N]A [N]G [N]G [N]T [N]G [N]G [N]T [N]A [N]C [N]C [N]A [N]G [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]A [N]T [N]C [N]C [N]A [N]G [N]A [N]A [N]C [N]T [N]G [N]C [N]C [N]C [N]A [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]G [N]C [N]T [N]C [N]A [N]A [N]A [N]C [N]T [N]C [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]C [N]A [N]C [N]C [N]T [N]G [N]C [N]C [N]A [N]G [N]C [N]T [N]G \n",
      "LAST SEP: 1023\n",
      "TOKEN CHUNK:  [N]T [N]T [N]T [N]A [N]A [N]C [N]T [N]G [N]A [N]G [N]A [N]A [N]A [N]A [N]T [N]G [N]C [N]A [N]T [N]G [N]T [N]A [N]A [N]G [N]T [N]G [N]C [N]T [N]C [N]A [N]T [N]C [N]C [N]T [N]A [N]A [N]T [N]A [N]C [N]A [N]A [N]A [N]A [N]T [N]A [N]A [N]A [N]T [N]G [N]G [N]T [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]T [N]A [N]T [N]T [N]A [N]T [N]T [N]T [N]A [N]A [N]T [N]T [N]T [N]T [N]T [N]A [N]A [N]A [N]C [N]A [N]G [N]C [N]C [N]C [N]T [N]G [N]C [N]C [N]G [N]G [N]T [N]T [N]A [N]G [N]G [N]C [N]A [N]T [N]T [N]A [N]C [N]C [N]A [N]T [N]C [N]C [N]T [N]C [N]A [N]T [N]T [N]T [N]C [N]A [N]C [N]A [N]G [N]A [N]T [N]G [N]A [N]G [N]G [N]C [N]A [N]G [N]T [N]T [N]G [N]T [N]T [N]G [N]C [N]A [N]C [N]A [N]G [N]G [N]T [N]G [N]G [N]T [N]A [N]C [N]C [N]A [N]G [N]G [N]A [N]T [N]T [N]A [N]G [N]A [N]A [N]T [N]C [N]C [N]A [N]G [N]A [N]A [N]C [N]T [N]G [N]C [N]C [N]C [N]A [N]A [N]C [N]C [N]C [N]T [N]G [N]G [N]G [N]G [N]C [N]T [N]C [N]A [N]A [N]A [N]C [N]T [N]C [N]T [N]T [N]C [N]C [N]T [N]A [N]C [N]C [N]A [N]C [N]C [N]T [N]G [N]C [N]C [N]A [N]G [N]C [N]T [N]G\n",
      "CHUNK TOKENS: ['[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]A', '[N]G', '[N]A', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]C', '[N]A', '[N]T', '[N]G', '[N]T', '[N]A', '[N]A', '[N]G', '[N]T', '[N]G', '[N]C', '[N]T', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]A', '[N]A', '[N]T', '[N]A', '[N]C', '[N]A', '[N]A', '[N]A', '[N]A', '[N]T', '[N]A', '[N]A', '[N]A', '[N]T', '[N]G', '[N]G', '[N]T', '[N]T', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]T', '[N]A', '[N]T', '[N]T', '[N]A', '[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]T', '[N]T', '[N]T', '[N]T', '[N]T', '[N]A', '[N]A', '[N]A', '[N]C', '[N]A', '[N]G', '[N]C', '[N]C', '[N]C', '[N]T', '[N]G', '[N]C', '[N]C', '[N]G', '[N]G', '[N]T', '[N]T', '[N]A', '[N]G', '[N]G', '[N]C', '[N]A', '[N]T', '[N]T', '[N]A', '[N]C', '[N]C', '[N]A', '[N]T', '[N]C', '[N]C', '[N]T', '[N]C', '[N]A', '[N]T', '[N]T', '[N]T', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]A', '[N]T', '[N]G', '[N]A', '[N]G', '[N]G', '[N]C', '[N]A', '[N]G', '[N]T', '[N]T', '[N]G', '[N]T', '[N]T', '[N]G', '[N]C', '[N]A', '[N]C', '[N]A', '[N]G', '[N]G', '[N]T', '[N]G', '[N]G', '[N]T', '[N]A', '[N]C', '[N]C', '[N]A', '[N]G', '[N]G', '[N]A', '[N]T', '[N]T', '[N]A', '[N]G', '[N]A', '[N]A', '[N]T', '[N]C', '[N]C', '[N]A', '[N]G', '[N]A', '[N]A', '[N]C', '[N]T', '[N]G', '[N]C', '[N]C', '[N]C', '[N]A', '[N]A', '[N]C', '[N]C', '[N]C', '[N]T', '[N]G', '[N]G', '[N]G', '[N]G', '[N]C', '[N]T', '[N]C', '[N]A', '[N]A', '[N]A', '[N]C', '[N]T', '[N]C', '[N]T', '[N]T', '[N]C', '[N]C', '[N]T', '[N]A', '[N]C', '[N]C', '[N]A', '[N]C', '[N]C', '[N]T', '[N]G', '[N]C', '[N]C', '[N]A', '[N]G', '[N]C', '[N]T', '[N]G']\n",
      "COUNTER: 5\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "test_tokens = []\n",
    "\n",
    "with all_annotation_file_paths[0].open('r') as f:\n",
    "    leftover_text = \"\"\n",
    "    counter = 0\n",
    "    while True:\n",
    "        print(f\"COUNTER: {counter}\")\n",
    "        token_string = f.read(1024)\n",
    "        if token_string == \"\" or counter == 5:\n",
    "            break\n",
    "        print(f\"LEFTOVER: {leftover_text}\")\n",
    "        print(f\"CHUNK: {token_string}\")\n",
    "        last_sep = token_string.rindex(\" \")\n",
    "        print(f\"LAST SEP: {last_sep}\")\n",
    "        if last_sep != -1:\n",
    "            # Get our chunk of uninterrupted tokens\n",
    "            token_chunk = leftover_text + token_string[:last_sep]\n",
    "            print(f\"TOKEN CHUNK: {token_chunk}\")\n",
    "            # Get our list of tokenized tokens\n",
    "            chunk_tokens = [tok for tok in token_chunk.split(\" \") if len(tok) > 0]\n",
    "            print(f\"CHUNK TOKENS: {chunk_tokens}\")\n",
    "            # Record the leftover string not in the token string\n",
    "            leftover_text = token_string[last_sep:]\n",
    "            # yield our list of tokens\n",
    "            test_tokens.append(chunk_tokens)\n",
    "        else:\n",
    "            leftover_text = leftover_text + token_string\n",
    "        counter += 1\n",
    "    test_tokens.append(leftover_text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037f4190-77c0-4c70-bb15-91594ffc5982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000003.12/100129480/gene.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_annotation_file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531807d-ec80-45c6-a322-9251fb97d084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1026"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(tok) for tok in test_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53ef94-1ce6-4003-ba1b-7dc794bccf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def yield_file_tokens(path: Path, chunksize=1024, sep=\" \") -> list[str]:\n",
    "    with path.open('r') as f:\n",
    "        leftover_text = \"\"\n",
    "        while True:\n",
    "            token_string = f.read(chunksize)\n",
    "            if token_string == \"\":\n",
    "                break\n",
    "            try:\n",
    "                last_sep = token_string.rindex(\" \")\n",
    "                # Get our chunk of uninterrupted tokens\n",
    "                token_chunk = leftover_text + token_string[:last_sep]\n",
    "                # Get our list of tokenized tokens\n",
    "                chunk_tokens = [tok for tok in token_chunk.split(\" \") if len(tok) > 0]\n",
    "                # Record the leftover string not in the token string\n",
    "                leftover_text = token_string[last_sep:]\n",
    "                # yield our list of tokens\n",
    "                yield chunk_tokens\n",
    "            except ValueError:\n",
    "                # If there is no sep, capture the string\n",
    "                leftover_text = leftover_text + token_string\n",
    "        chunk_tokens = [tok for tok in leftover_text.split(\" \") if len(tok) > 0]\n",
    "        if len(chunk_tokens) > 0:\n",
    "            yield chunk_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e08ff-7b7d-4517-8145-dc018787d4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21224"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "test_file_tokens = list(yield_file_tokens(all_annotation_file_paths[0]))\n",
    "sum([len(token_list) for token_list in test_file_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917e64b-c53b-4849-ae04-28a8ec1b79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Sort file paths by size\n",
    "def get_sequence_length(path: Path):\n",
    "    length = 0\n",
    "    for tokens in yield_file_tokens(path):\n",
    "        length += len(tokens)\n",
    "    return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60eadf-f036-47b8-a946-d3bc8908c778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21224"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "get_sequence_length(all_annotation_file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5500d7-c4dc-427d-af87-1a3d6a94fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "path_file_sizes = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11af12-503b-4b35-b089-9f5fd99bb110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02f3b04adf14aa18888ff8de39e3dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "def get_sequence_length_mp(path: Path):\n",
    "    return path, get_sequence_length(path)\n",
    "\n",
    "\n",
    "pbar = tqdm(total=len(gene_file_paths))\n",
    "with Pool(os.cpu_count() - 1) as p:\n",
    "    for result in p.imap_unordered(get_sequence_length_mp, gene_file_paths):\n",
    "        result_path, result_length = result\n",
    "        path_file_sizes[result_path] = result_length\n",
    "        pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69e41f-128c-487b-b465-ed721e925df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "      <th>gene</th>\n",
       "      <th>gene_annotation</th>\n",
       "      <th>protein_annotation</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75563</th>\n",
       "      <td>NC_000016</td>\n",
       "      <td>54715</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-XM_017023318.3.txt</td>\n",
       "      <td>2473620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75550</th>\n",
       "      <td>NC_000016</td>\n",
       "      <td>54715</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415912.1.txt</td>\n",
       "      <td>2473620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75548</th>\n",
       "      <td>NC_000016</td>\n",
       "      <td>54715</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415910.1.txt</td>\n",
       "      <td>2473620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75549</th>\n",
       "      <td>NC_000016</td>\n",
       "      <td>54715</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415911.1.txt</td>\n",
       "      <td>2473620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75564</th>\n",
       "      <td>NC_000016</td>\n",
       "      <td>54715</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-XM_017023320.3.txt</td>\n",
       "      <td>2473620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      annotation   gene   \n",
       "75563  NC_000016  54715  \\\n",
       "75550  NC_000016  54715   \n",
       "75548  NC_000016  54715   \n",
       "75549  NC_000016  54715   \n",
       "75564  NC_000016  54715   \n",
       "\n",
       "                                                                   gene_annotation   \n",
       "75563  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt  \\\n",
       "75550  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt   \n",
       "75548  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt   \n",
       "75549  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt   \n",
       "75564  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt   \n",
       "\n",
       "                                                                              protein_annotation   \n",
       "75563  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-XM_017023318.3.txt  \\\n",
       "75550  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415912.1.txt   \n",
       "75548  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415910.1.txt   \n",
       "75549  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415911.1.txt   \n",
       "75564  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-XM_017023320.3.txt   \n",
       "\n",
       "          size  \n",
       "75563  2473620  \n",
       "75550  2473620  \n",
       "75548  2473620  \n",
       "75549  2473620  \n",
       "75564  2473620  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "# Get sorted list of annotation paths by size\n",
    "# Protein to gene lookup\n",
    "training_paths_with_size = training_paths.copy()\n",
    "training_paths_with_size.loc[:, 'size'] = training_paths_with_size.gene_annotation.apply(lambda path_str: path_file_sizes.get(Path(path_str)))\n",
    "# Sort by file size via gene to file size lookup\n",
    "training_paths_with_size.sort_values('size', ascending=False, inplace=True)\n",
    "training_paths_with_size.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d247a3-91fc-4c95-8636-01c57cc2cb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2473620</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106844</th>\n",
       "      <td>2473620</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-XM_017023318.3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106872</th>\n",
       "      <td>2473620</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415909.1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106873</th>\n",
       "      <td>2473620</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415899.1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106874</th>\n",
       "      <td>2473620</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415907.1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           size   \n",
       "0       2473620  \\\n",
       "106844  2473620   \n",
       "106872  2473620   \n",
       "106873  2473620   \n",
       "106874  2473620   \n",
       "\n",
       "                                                                                            value  \n",
       "0                     /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/gene.txt  \n",
       "106844  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-XM_017023318.3.txt  \n",
       "106872  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415909.1.txt  \n",
       "106873  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415899.1.txt  \n",
       "106874  /mnt/e/Data/llm-mito-scanner-data/data/training/NC_000016.10/54715/rna-NM_001415907.1.txt  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "all_annotation_file_paths_sorted = pd.melt(\n",
    "    training_paths_with_size, \n",
    "    id_vars=['size'], \n",
    "    value_vars=['gene_annotation', 'protein_annotation']\n",
    ").drop(\n",
    "    \"variable\", \n",
    "    axis=1\n",
    ").drop_duplicates(\n",
    "    subset=['value']\n",
    ").sort_values('size', ascending=False).dropna(subset=[\"value\"])#.apply(Path)\n",
    "all_annotation_file_paths_sorted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81eea0-68c2-4959-beb0-2c20b192ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_batches(batch_min_size: int, path_df: pd.DataFrame):\n",
    "    batches = []\n",
    "    batch = []\n",
    "    batch_size = 0\n",
    "    pbar = tqdm(total=path_df.shape[0])\n",
    "    for idx, row in path_df.iterrows():\n",
    "        row_path = Path(row.value)\n",
    "        if batch_size >= batch_min_size:\n",
    "            batches.append(batch)\n",
    "            batch = [row_path]\n",
    "            batch_size = row['size']\n",
    "        else:\n",
    "            batch.append(row_path)\n",
    "            batch_size += row['size']\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    batches.append(batch)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edca563-bbd8-4b8c-b63a-1f2d15148f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5826df0e9145ec8273e0ea9ed51e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/123138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "file_batches = make_batches(1000000, all_annotation_file_paths_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1bf578-e261-42e1-9a81-497e06c811b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11966"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "len(file_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84af59a-6cad-40e9-a870-11bd13173ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11966.000000\n",
       "mean        10.290657\n",
       "std         22.495713\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          5.000000\n",
       "75%         10.000000\n",
       "max        801.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "pd.Series([len(batch) for batch in file_batches]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44338d93-a70c-4365-9481-ed7da264b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from torchtext.vocab import Vocab\n",
    "from collections import Counter, OrderedDict\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import numpy as np\n",
    "from operator import attrgetter\n",
    "from functools import reduce\n",
    "from operator import add\n",
    "import itertools\n",
    "\n",
    "\n",
    "def build_vocab(file_paths: list[Path], special_tokens: list[str] = ['<unk>']) -> Vocab:\n",
    "    # Terms in the vocab will be ordered by the order they're added to this counter\n",
    "    counter = Counter()\n",
    "    # Add common nucleotides\n",
    "    nucleotide_tag = \"[N]\"\n",
    "    nucleotides = list(\"ACGT\")\n",
    "    nucleotide_tokens = [\n",
    "        f\"{nucleotide_tag}{nucleotide}\" for nucleotide in nucleotides\n",
    "    ]\n",
    "    counter.update(nucleotide_tokens)\n",
    "    # mRNA tags\n",
    "    counter.update(['[intron]', '[exon]'])\n",
    "    # amino acids\n",
    "    amino_acids = list(\"ARNDCQEGHILKMFPSTWYVBZX\")\n",
    "    amino_acid_tag = \"[A]\"\n",
    "    codon_pos = list(range(1, 4))\n",
    "    amino_acid_tokens = [\n",
    "        f\"{amino_acid_tag}-{amino_acid_val}-{codon_pos_val}\" for amino_acid_val, codon_pos_val in itertools.product(\n",
    "            amino_acids, \n",
    "            codon_pos\n",
    "        )\n",
    "    ]\n",
    "    counter.update(amino_acid_tokens)\n",
    "    # Rare nucleotides\n",
    "    rare_nucleotides = list(\"URYKMSWBDHVN\")\n",
    "    rare_nucleotide_tokens = [\n",
    "        f\"{nucleotide_tag}-{nucleotide}\" for nucleotide in rare_nucleotides\n",
    "    ]\n",
    "    counter.update(rare_nucleotide_tokens)\n",
    "    token_vocab = vocab(counter, specials=special_tokens)\n",
    "    return token_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350a7a0-6382-4b00-8354-c78067c3b79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "test_vocab = build_vocab(file_batches[-3:])\n",
    "test_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943f1c7-fdd5-4c90-9fb0-84f60349cc2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 0, 43)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "test_vocab[\"[N]T\"], test_vocab[\"[N]A\"], test_vocab[\"<unk>\"], test_vocab[\"[A]-M-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fd32cc-6879-47e6-b5b2-64bdd4e79a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Save the vocabulary\n",
    "artefacts_path = data_path / \"artefacts\"\n",
    "training_artefacts_path = artefacts_path / \"training\"\n",
    "if not training_artefacts_path.exists():\n",
    "    training_artefacts_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14992ac9-884b-4740-a763-c145e4c843e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch\n",
    "\n",
    "vocab_path = training_artefacts_path / \"vocab.pt\"\n",
    "if not vocab_path.exists():\n",
    "    vocab = build_vocab(file_batches)\n",
    "    torch.save(vocab, vocab_path)\n",
    "else:\n",
    "    vocab = torch.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e06c7e-a0bd-4553-b26d-e94f6f936b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 2, 3, 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "vocab[\"[N]T\"], vocab[\"[N]A\"], vocab[\"[N]C\"], vocab[\"[N]G\"], vocab[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820a5f46-1270-4753-9fef-7e893963d1f3",
   "metadata": {},
   "source": [
    "### Build our training, validation, test indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c85be0-f3c4-42db-b40e-895a78cbf138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Build indices for train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898ad26-f536-4259-987b-37406da51d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9946d-8338-4b74-bc99-d18050b00447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ``train_iter`` was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape ``[seq_len, batch_size]``\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcc009b-0f0a-4364-9530-68a2784f8c0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5113515-92c2-4825-b7f6-5f972384d5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
