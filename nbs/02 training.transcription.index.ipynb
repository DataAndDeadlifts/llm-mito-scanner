{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.transcription.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages/Bio/__init__.py:138: BiopythonWarning: You may be importing Biopython from inside the source tree. This is bad practice and might lead to downstream issues. In particular, you might encounter ImportErrors due to missing compiled C extensions. We recommend that you try running your code from outside the source tree. If you are outside the source tree then you have a setup.py file in an unexpected directory: /home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import vocab, Vocab\n",
    "from collections import Counter, OrderedDict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from llm_mito_scanner.data.download import load_config, \\\n",
    "    get_latest_assembly_path, get_genomic_genbank_path\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "data_path = Path(config.get(\"data_path\"))\n",
    "data_raw_path = data_path / \"raw\"\n",
    "assemblies_path = data_raw_path / \"assemblies\"\n",
    "latest_assembly_path = get_latest_assembly_path(assemblies_path)\n",
    "genomic_genbank_path = get_genomic_genbank_path(latest_assembly_path)\n",
    "chromosomes_path = latest_assembly_path / \"chromosomes\"\n",
    "training_data_path = latest_assembly_path / \"training\"\n",
    "transcription_data_path = training_data_path / \"transcription\"\n",
    "sequences_data_path = transcription_data_path / \"sequences\"\n",
    "if not sequences_data_path.exists():\n",
    "    raise Exception(f\"This notebook expects the path at {sequences_data_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/mnt/e/Data/llm-mito-scanner-data/data/raw/assemblies/GCF_000001405.40_GRCh38.p14/training/transcription/sequences/chromosome=NC_000001.11/partition=0/sequences.parquet'),\n",
       " PosixPath('/mnt/e/Data/llm-mito-scanner-data/data/raw/assemblies/GCF_000001405.40_GRCh38.p14/training/transcription/sequences/chromosome=NC_000001.11/partition=1/sequences.parquet'),\n",
       " PosixPath('/mnt/e/Data/llm-mito-scanner-data/data/raw/assemblies/GCF_000001405.40_GRCh38.p14/training/transcription/sequences/chromosome=NC_000001.11/partition=10/sequences.parquet')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "chromosome_parquet_files = list(sequences_data_path.glob(\"chromosome=*/partition=*/*.parquet\"))\n",
    "chromosome_parquet_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "parquet_file_df = pd.DataFrame(chromosome_parquet_files, columns=['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "      <td>NC_000001.11</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path    chromosome  partition\n",
       "0  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  NC_000001.11          0\n",
       "1  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  NC_000001.11          1\n",
       "2  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  NC_000001.11          2\n",
       "3  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  NC_000001.11          3\n",
       "4  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  NC_000001.11          4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "parquet_file_df_path_split = parquet_file_df.path.apply(lambda p: str(p).rsplit(\"/\"))\n",
    "\n",
    "parquet_file_df.loc[:, 'chromosome'] = parquet_file_df_path_split.apply(lambda split_path: split_path[-3].split(\"=\")[-1])\n",
    "parquet_file_df.loc[:, 'partition'] = parquet_file_df_path_split.apply(lambda split_path: int(split_path[-2].split(\"=\")[-1]))\n",
    "parquet_file_df.sort_values(['chromosome', 'partition'], inplace=True)\n",
    "parquet_file_df.reset_index(drop=True, inplace=True)\n",
    "parquet_file_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def index_training_sequence_files(sequences_path: Path) -> pd.DataFrame:\n",
    "    # Index files\n",
    "    chromosome_parquet_files = list(sequences_path.glob(\"chromosome=*/partition=*/*.parquet\"))\n",
    "    parquet_file_df = pd.DataFrame(chromosome_parquet_files, columns=['path'])\n",
    "    # Extract partition info\n",
    "    parquet_file_df_path_split = parquet_file_df.path.apply(lambda p: str(p).rsplit(\"/\"))\n",
    "    parquet_file_df.loc[:, 'chromosome'] = parquet_file_df_path_split.apply(lambda split_path: split_path[-3].split(\"=\")[-1])\n",
    "    parquet_file_df.loc[:, 'partition'] = parquet_file_df_path_split.apply(lambda split_path: int(split_path[-2].split(\"=\")[-1]))\n",
    "    # Sort\n",
    "    parquet_file_df.sort_values(['chromosome', 'partition'], inplace=True)\n",
    "    parquet_file_df.reset_index(drop=True, inplace=True)\n",
    "    return parquet_file_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_training_index(index_dir: Path, sequences_path: Path, sample: bool = False, save: bool = False):\n",
    "    sequence_files = index_training_sequence_files(sequences_path)\n",
    "    if sample:\n",
    "        sequence_files = sequence_files.tail(2)\n",
    "    sequences = sequence_files.path.tolist()\n",
    "    frames = []\n",
    "    for f in tqdm(sequences):\n",
    "        f_frame = pd.read_parquet(f, columns=[\"geneid\", 'transcriptid']).reset_index(drop=False).rename({\"index\": \"file_index\"}, axis=1)\n",
    "        f_frame.loc[:, 'file'] = f\n",
    "        frames.append(f_frame)\n",
    "    training_data_index = pd.concat(\n",
    "        frames, \n",
    "        axis=0, ignore_index=True\n",
    "    ).reset_index(drop=True)\n",
    "    if save:\n",
    "        training_data_index.to_csv(index_dir / \"index.csv\", index=False)\n",
    "    return training_data_index\n",
    "\n",
    "\n",
    "def get_training_index(index_dir: Path, sequences_path: Path = None, **make_kwargs):\n",
    "    index_path = index_dir / \"index.csv\"\n",
    "    if not index_path.exists():\n",
    "        return make_training_index(index_dir, sequences_path, **make_kwargs)\n",
    "    else:\n",
    "        return pd.read_csv(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "example_training_data_index = get_training_index(transcription_data_path, sequences_data_path, save=False, sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_index</th>\n",
       "      <th>geneid</th>\n",
       "      <th>transcriptid</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GeneID:79501</td>\n",
       "      <td>NM_001005484.2</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GeneID:112268260</td>\n",
       "      <td>XM_047436352.1</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GeneID:729759</td>\n",
       "      <td>NM_001005221.2</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GeneID:105378947</td>\n",
       "      <td>XM_011542538.1</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GeneID:81399</td>\n",
       "      <td>XM_024449992.2</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_index            geneid    transcriptid   \n",
       "0           0      GeneID:79501  NM_001005484.2  \\\n",
       "1           1  GeneID:112268260  XM_047436352.1   \n",
       "2           2     GeneID:729759  NM_001005221.2   \n",
       "3           3  GeneID:105378947  XM_011542538.1   \n",
       "4           4      GeneID:81399  XM_024449992.2   \n",
       "\n",
       "                                                file  \n",
       "0  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "1  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "2  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "3  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "4  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "example_training_data_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_train_test_split(index: pd.DataFrame, random_state = 42):\n",
    "    return train_test_split(index, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "train_idx, test_idx = make_train_test_split(example_training_data_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97815, 4), (32605, 4))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_index</th>\n",
       "      <th>geneid</th>\n",
       "      <th>transcriptid</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106188</th>\n",
       "      <td>105</td>\n",
       "      <td>GeneID:5889</td>\n",
       "      <td>XM_047436505.1</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124266</th>\n",
       "      <td>434</td>\n",
       "      <td>GeneID:9814</td>\n",
       "      <td>NM_001258327.2</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91738</th>\n",
       "      <td>62</td>\n",
       "      <td>GeneID:81614</td>\n",
       "      <td>XM_011543879.4</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41879</th>\n",
       "      <td>118</td>\n",
       "      <td>GeneID:63027</td>\n",
       "      <td>NM_021945.6</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108927</th>\n",
       "      <td>164</td>\n",
       "      <td>GeneID:2774</td>\n",
       "      <td>NM_001142339.3</td>\n",
       "      <td>/mnt/e/Data/llm-mito-scanner-data/data/raw/ass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_index        geneid    transcriptid   \n",
       "106188         105   GeneID:5889  XM_047436505.1  \\\n",
       "124266         434   GeneID:9814  NM_001258327.2   \n",
       "91738           62  GeneID:81614  XM_011543879.4   \n",
       "41879          118  GeneID:63027     NM_021945.6   \n",
       "108927         164   GeneID:2774  NM_001142339.3   \n",
       "\n",
       "                                                     file  \n",
       "106188  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "124266  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "91738   /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "41879   /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  \n",
       "108927  /mnt/e/Data/llm-mito-scanner-data/data/raw/ass...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "train_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sequence(file: Path, idx: int) -> pd.Series:\n",
    "    row = pd.read_parquet(file).iloc[idx, :]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "example_sequence = get_sequence(train_idx.iloc[0, -1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428166"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "example_sequence.input.count(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TranscriptionDataset(Dataset):\n",
    "    def __init__(self, training_path: Path, sequences_path: Path, train: bool):\n",
    "        self.training_path = training_path\n",
    "        self.sequences_path = sequences_path\n",
    "        self.train = train\n",
    "        self.training_index = get_training_index(self.training_path, self.sequences_path, sample=False, save=True)\n",
    "        self.train_idx, self.test_idx = make_train_test_split(self.training_index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.train:\n",
    "            return self.train_idx.shape[0]\n",
    "        else:\n",
    "            return self.test_idx.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[str, str]:\n",
    "        if self.train:\n",
    "            sequence_row = self.train_idx.iloc[idx, :]\n",
    "        else:\n",
    "            sequence_row = self.test_idx.iloc[idx, :]\n",
    "        sequence = get_sequence(sequence_row.file, sequence_row.file_index)\n",
    "        return sequence.input, sequence.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "transcription_dataset_train = TranscriptionDataset(transcription_data_path, sequences_data_path, True)\n",
    "transcription_dataset_test = TranscriptionDataset(transcription_data_path, sequences_data_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97815, 32605)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "len(transcription_dataset_train), len(transcription_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13415\n",
      "122052\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "for i, tup in enumerate(transcription_dataset_train):\n",
    "    if i == 2:\n",
    "        break\n",
    "    print(tup[0].count(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize(seq: str) -> list[str]:\n",
    "    return seq.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sample_train_data = transcription_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13416, 13416)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "len(tokenize(sample_train_data[0])), len(tokenize(sample_train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "gene_path = latest_assembly_path / \"genes\"\n",
    "gene_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'C': 1, 'D': 1})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "Counter(\"C,D\".split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def count_transcription_tokens(parquet_path: Path) -> Counter:\n",
    "    token_counter = Counter()\n",
    "    sequences = pd.read_parquet(parquet_path, columns=['input', 'target'])\n",
    "    input_counter = sum(sequences.input.apply(tokenize).apply(Counter).values.tolist(), Counter())\n",
    "    target_counter = sum(sequences.target.apply(tokenize).apply(Counter).values.tolist(), Counter())\n",
    "    token_counter = input_counter + target_counter\n",
    "    return token_counter\n",
    "\n",
    "\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "SPECIAL_TOKENS = [\n",
    "    UNK_TOKEN,\n",
    "    PAD_TOKEN,\n",
    "    BOS_TOKEN,\n",
    "    EOS_TOKEN\n",
    "]\n",
    "\n",
    "\n",
    "def build_vocab(\n",
    "        parquet_files: list[Path], \n",
    "        special_tokens: list[str] = SPECIAL_TOKENS,\n",
    "        unknown_token: str = UNK_TOKEN):\n",
    "    counter = Counter()\n",
    "    max_processes = min(8, os.cpu_count() - 1)\n",
    "    pool = Pool(\n",
    "        processes=min(max_processes, len(parquet_files)))\n",
    "    try:\n",
    "        pbar = tqdm(total=len(parquet_files), leave=False)\n",
    "        for c in pool.imap_unordered(count_transcription_tokens, parquet_files):\n",
    "            counter = counter + c\n",
    "            pbar.update(1)\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    finally:\n",
    "        pbar.close()\n",
    "        pool.close()\n",
    "    token_ordered_dict = OrderedDict(counter.most_common())\n",
    "    transcription_vocab = vocab(token_ordered_dict, specials=special_tokens, special_first=True)\n",
    "    unk_index = transcription_vocab[unknown_token]\n",
    "    transcription_vocab.set_default_index(unk_index)\n",
    "    return transcription_vocab\n",
    "\n",
    "\n",
    "def get_vocab(\n",
    "        training_path: Path,\n",
    "        force_build: bool = False,\n",
    "        save: bool = True,\n",
    "        **build_vocab_kwargs\n",
    "        ) -> Vocab:\n",
    "    vocab_path = training_path / \"vocab.pt\"\n",
    "    if not vocab_path.exists() or force_build:\n",
    "        transcription_vocab = build_vocab(**build_vocab_kwargs)\n",
    "        if save:\n",
    "            torch.save(transcription_vocab, vocab_path)\n",
    "    else:\n",
    "        transcription_vocab = torch.load(vocab_path)\n",
    "    return transcription_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "example_transcription_vocab = get_vocab(\n",
    "    transcription_data_path, \n",
    "    force_build=True, \n",
    "    save=False,\n",
    "    parquet_files=chromosome_parquet_files[-2:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "len(example_transcription_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<bos>', '<eos>', '<intron>', 'T', 'A', 'G', 'C']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "example_transcription_vocab.lookup_tokens(list(range(0, len(example_transcription_vocab))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_training_sequence(sequence: tuple[str, str], data_vocab: Vocab) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    input_tensor = torch.tensor(data_vocab(tokenize(sequence[0])), dtype=torch.long)\n",
    "    target_tensor = torch.tensor(data_vocab(tokenize(sequence[1])), dtype=torch.long)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7, 5, 7,  ..., 7, 6, 6]), tensor([7, 5, 7,  ..., 7, 6, 6]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "processed_example_sequence = process_training_sequence(sample_train_data, example_transcription_vocab)\n",
    "processed_example_sequence[0], processed_example_sequence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13416])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "processed_example_sequence[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def batchify_sequence(sequence: Tensor, bsz: int) -> Tensor:\n",
    "    global device\n",
    "    seq_len = sequence.size(0) // bsz\n",
    "    data = sequence[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def batchify(sequence_tensors: tuple[Tensor, Tensor], bsz: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    input_batches = batchify_sequence(sequence_tensors[0], bsz)\n",
    "    target_batches = batchify_sequence(sequence_tensors[1], bsz)\n",
    "    return input_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_batch(input: Tensor, target: Tensor, i: int, bptt: int = 35) -> tuple[Tensor, Tensor]:\n",
    "    global device\n",
    "    seq_len = min(bptt, len(input) - 1 - i)\n",
    "    data = input[i:i+seq_len].to(device)\n",
    "    target = target[i:i+seq_len].to(device)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "bptt = 35\n",
    "eval_batch_size = 10\n",
    "\n",
    "processed_example_sequence_input, processed_example_sequence_target = processed_example_sequence\n",
    "sequences_batch_num = (processed_example_sequence_input.shape[0] - 1) // bptt\n",
    "for batch, i in enumerate(range(0, sequences_batch_num, bptt)):\n",
    "    # Get Batch\n",
    "    data, target = get_batch(processed_example_sequence_input, processed_example_sequence_target, i, bptt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([7, 5, 7, 8, 7, 7, 6, 7, 5, 5, 5, 7, 7, 8, 5, 7, 8, 5, 8, 8, 7, 7, 7, 7,\n",
       "         5, 5, 6, 7, 8, 6, 7, 7, 5, 7, 6], device='cuda:0'),\n",
       " tensor([7, 5, 7, 8, 7, 7, 6, 7, 5, 5, 5, 7, 7, 8, 5, 7, 8, 5, 8, 8, 7, 7, 7, 7,\n",
       "         5, 5, 6, 7, 8, 6, 7, 7, 5, 7, 6], device='cuda:0'),\n",
       " torch.Size([35]),\n",
       " torch.Size([35]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "print(sequences_batch_num)\n",
    "data, target, data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
