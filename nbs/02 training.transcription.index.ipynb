{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp training.transcription.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages/Bio/__init__.py:138: BiopythonWarning: You may be importing Biopython from inside the source tree. This is bad practice and might lead to downstream issues. In particular, you might encounter ImportErrors due to missing compiled C extensions. We recommend that you try running your code from outside the source tree. If you are outside the source tree then you have a setup.py file in an unexpected directory: /home/jdb/projects/llm-mito-scanner/venv/lib/python3.10/site-packages\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, IterableDataset\n",
    "from torchtext.vocab import vocab, Vocab\n",
    "from collections import Counter, OrderedDict\n",
    "import operator\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "from llm_mito_scanner.data.download import load_config, \\\n",
    "    get_latest_assembly_path, get_genomic_genbank_path\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "data_path = Path(config.get(\"data_path\"))\n",
    "data_raw_path = data_path / \"raw\"\n",
    "assemblies_path = data_raw_path / \"assemblies\"\n",
    "latest_assembly_path = get_latest_assembly_path(assemblies_path)\n",
    "genomic_genbank_path = get_genomic_genbank_path(latest_assembly_path)\n",
    "chromosomes_path = latest_assembly_path / \"chromosomes\"\n",
    "training_data_path = latest_assembly_path / \"training\"\n",
    "transcription_data_path = training_data_path / \"transcription\"\n",
    "sequences_data_path = transcription_data_path / \"sequences\"\n",
    "if not sequences_data_path.exists():\n",
    "    raise Exception(f\"This notebook expects the path at {sequences_data_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_training_index(index_dir: Path, sequences_path: Path, sample: bool = False, save: bool = False):\n",
    "    training_data_index_frames = []\n",
    "    sequences = list(sequences_path.glob(\"*.csv\"))\n",
    "    if sample:\n",
    "        sequences = sequences[-2:]\n",
    "    for f in tqdm(sequences):\n",
    "        f_frame = pd.read_csv(f, usecols=[\"geneid\"]).reset_index(drop=False).rename({\"index\": \"file_index\"}, axis=1)\n",
    "        f_frame.loc[:, 'file'] = f.stem\n",
    "        training_data_index_frames.append(f_frame)\n",
    "    training_data_index = pd.concat(\n",
    "        training_data_index_frames, \n",
    "        axis=0, ignore_index=True\n",
    "    ).reset_index(drop=True)\n",
    "    if save:\n",
    "        training_data_index.to_csv(index_dir / \"index.csv\", index=False)\n",
    "    return training_data_index\n",
    "\n",
    "\n",
    "def get_training_index(index_dir: Path, sequences_path: Path = None, **make_kwargs):\n",
    "    index_path = index_dir / \"index.csv\"\n",
    "    if not index_path.exists():\n",
    "        return make_training_index(index_dir, sequences_path, **make_kwargs)\n",
    "    else:\n",
    "        return pd.read_csv(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "training_data_index = get_training_index(transcription_data_path, sequences_data_path, save=False, sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_index</th>\n",
       "      <th>geneid</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>GeneID:79501</td>\n",
       "      <td>NC_000001.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>GeneID:112268260</td>\n",
       "      <td>NC_000001.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GeneID:729759</td>\n",
       "      <td>NC_000001.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>GeneID:105378947</td>\n",
       "      <td>NC_000001.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GeneID:81399</td>\n",
       "      <td>NC_000001.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_index            geneid          file\n",
       "0           0      GeneID:79501  NC_000001.11\n",
       "1           1  GeneID:112268260  NC_000001.11\n",
       "2           2     GeneID:729759  NC_000001.11\n",
       "3           3  GeneID:105378947  NC_000001.11\n",
       "4           4      GeneID:81399  NC_000001.11"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "training_data_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def make_train_test_split(index: pd.DataFrame, random_state = 42):\n",
    "    return train_test_split(index, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "train_idx, test_idx = make_train_test_split(training_data_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((97815, 3), (32605, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "train_idx.shape, test_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_index</th>\n",
       "      <th>geneid</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106188</th>\n",
       "      <td>5022</td>\n",
       "      <td>GeneID:5889</td>\n",
       "      <td>NC_000017.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124266</th>\n",
       "      <td>1229</td>\n",
       "      <td>GeneID:9814</td>\n",
       "      <td>NC_000022.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91738</th>\n",
       "      <td>62</td>\n",
       "      <td>GeneID:81614</td>\n",
       "      <td>NC_000015.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41879</th>\n",
       "      <td>118</td>\n",
       "      <td>GeneID:63027</td>\n",
       "      <td>NC_000006.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108927</th>\n",
       "      <td>432</td>\n",
       "      <td>GeneID:2774</td>\n",
       "      <td>NC_000018.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_index        geneid          file\n",
       "106188        5022   GeneID:5889  NC_000017.11\n",
       "124266        1229   GeneID:9814  NC_000022.11\n",
       "91738           62  GeneID:81614  NC_000015.10\n",
       "41879          118  GeneID:63027  NC_000006.12\n",
       "108927         432   GeneID:2774  NC_000018.10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "train_idx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sequence(sequences_path: Path, file: str, idx: int) -> pd.Series:\n",
    "    row = pd.read_csv(sequences_path / f\"{file}.csv\", header=None, skiprows=idx, nrows=1)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "example_sequence = get_sequence(sequences_data_path, \"NC_000023.11\", 3403)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27639"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "example_sequence.iloc[0, 1].count(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TranscriptionDataset(Dataset):\n",
    "    def __init__(self, training_path: Path, sequences_path: Path, train: bool):\n",
    "        self.training_path = training_path\n",
    "        self.sequences_path = sequences_path\n",
    "        self.train = train\n",
    "        self.training_index = get_training_index(self.training_path, self.sequences_path, sample=False, save=True)\n",
    "        self.train_idx, self.test_idx = make_train_test_split(self.training_index)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.train:\n",
    "            return self.train_idx.shape[0]\n",
    "        else:\n",
    "            return self.test_idx.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple[str, str]:\n",
    "        if self.train:\n",
    "            sequence_row = self.train_idx.iloc[idx, :]\n",
    "        else:\n",
    "            sequence_row = self.test_idx.iloc[idx, :]\n",
    "        sequence_file_stem = sequence_row.file\n",
    "        sequence_file_idx = sequence_row.file_index\n",
    "        sequence = get_sequence(sequences_data_path, sequence_file_stem, sequence_file_idx)\n",
    "        sequence_input = sequence.iloc[0, 1]\n",
    "        sequence_target = sequence.iloc[0, 2]\n",
    "        return sequence_input, sequence_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "transcription_dataset_train = TranscriptionDataset(transcription_data_path, sequences_data_path, True)\n",
    "transcription_dataset_test = TranscriptionDataset(transcription_data_path, sequences_data_path, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97815, 32605)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "len(transcription_dataset_train), len(transcription_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43009\n",
      "122052\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "for i, tup in enumerate(transcription_dataset_train):\n",
    "    if i == 2:\n",
    "        break\n",
    "    print(tup[0].count(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def tokenize(seq: str) -> list[str]:\n",
    "    return seq.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "sample_train_data = transcription_dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43010, 43010)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "len(tokenize(sample_train_data[0])), len(tokenize(sample_train_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "gene_path = latest_assembly_path / \"genes\"\n",
    "gene_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def count_transcription_tokens(genes_path: Path, pbar: bool = False) -> OrderedDict:\n",
    "    token_counter = Counter()\n",
    "    gene_files = list(genes_path.glob(\"*.csv\"))\n",
    "    if pbar:\n",
    "        gene_files = tqdm(gene_files, leave=False, position=0)\n",
    "    for f in gene_files:\n",
    "        f_sequences = pd.read_csv(f, usecols=['sequence']).sequence\n",
    "        if pbar:\n",
    "            f_sequences = tqdm(f_sequences, leave=False, position=1)\n",
    "        f_counter = sum(map(Counter, f_sequences), Counter())\n",
    "        token_counter = token_counter + f_counter\n",
    "    token_ordered_dict = OrderedDict(token_counter.most_common())\n",
    "    return token_ordered_dict\n",
    "\n",
    "\n",
    "def build_vocab(\n",
    "        genes_path: Path, \n",
    "        pbar: bool = False, \n",
    "        intron_token: str = \"<intron>\", unknown_token: str = \"<unk>\", \n",
    "        default_index: int = -1):\n",
    "    token_ordered_dict = count_transcription_tokens(genes_path=genes_path, pbar=pbar)\n",
    "    transcription_vocab = vocab(token_ordered_dict, specials=[intron_token, unknown_token])\n",
    "    transcription_vocab.set_default_index(default_index)\n",
    "    return transcription_vocab\n",
    "\n",
    "\n",
    "def get_vocab(\n",
    "        training_path: Path,\n",
    "        **build_vocab_kwargs\n",
    "        ) -> Vocab:\n",
    "    vocab_path = training_path / \"vocab.pt\"\n",
    "    if not vocab_path.exists():\n",
    "        transcription_vocab = build_vocab(**build_vocab_kwargs)\n",
    "        torch.save(transcription_vocab, vocab_path)\n",
    "    else:\n",
    "        transcription_vocab = torch.load(vocab_path)\n",
    "    return transcription_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "transcription_vocab = get_vocab(transcription_data_path, pbar=True, genes_path=gene_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "len(transcription_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def process_training_sequence(sequence: tuple[str, str], data_vocab: Vocab) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    input_tensor = torch.tensor(data_vocab(tokenize(sequence[0])), dtype=torch.long)\n",
    "    target_tensor = torch.tensor(data_vocab(tokenize(sequence[1])), dtype=torch.long)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4, 2, 4,  ..., 5, 2, 5]), tensor([4, 2, 4,  ..., 5, 2, 5]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "processed_example_sequence = process_training_sequence(sample_train_data, transcription_vocab)\n",
    "processed_example_sequence[0], processed_example_sequence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43010])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "processed_example_sequence[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def batchify_sequence(sequence: Tensor, bsz: int) -> Tensor:\n",
    "    global device\n",
    "    seq_len = sequence.size(0) // bsz\n",
    "    data = sequence[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def batchify(sequence_tensors: tuple[Tensor, Tensor], bsz: int) -> tuple[Tensor, Tensor]:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    input_batches = batchify_sequence(sequence_tensors[0], bsz)\n",
    "    target_batches = batchify_sequence(sequence_tensors[1], bsz)\n",
    "    return input_batches, target_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4, 4, 3,  ..., 5, 5, 2],\n",
       "         [2, 4, 3,  ..., 3, 2, 3],\n",
       "         [4, 3, 4,  ..., 5, 4, 3],\n",
       "         ...,\n",
       "         [2, 5, 3,  ..., 5, 2, 4],\n",
       "         [4, 2, 3,  ..., 3, 4, 3],\n",
       "         [3, 4, 2,  ..., 5, 3, 2]], device='cuda:0'),\n",
       " tensor([[4, 0, 0,  ..., 5, 5, 2],\n",
       "         [2, 0, 0,  ..., 3, 2, 3],\n",
       "         [4, 0, 0,  ..., 5, 4, 3],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 5, 2, 4],\n",
       "         [0, 0, 0,  ..., 3, 4, 3],\n",
       "         [0, 0, 0,  ..., 5, 3, 2]], device='cuda:0'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "example_sequence_batchified = batchify(processed_example_sequence, batch_size)\n",
    "example_sequence_batchified[0], example_sequence_batchified[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([430, 100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "example_sequence_batchified[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_batch(input: Tensor, target: Tensor, i: int, bptt: int = 35) -> tuple[Tensor, Tensor]:\n",
    "    global device\n",
    "    seq_len = min(bptt, len(input) - 1 - i)\n",
    "    data = input[i:i+seq_len].to(device)\n",
    "    target = target[i:i+seq_len].to(device)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "bptt = 35\n",
    "eval_batch_size = 10\n",
    "\n",
    "processed_example_sequence_input, processed_example_sequence_target = processed_example_sequence\n",
    "sequences_batch_num = (processed_example_sequence_input.shape[0] - 1) // bptt\n",
    "for batch, i in enumerate(range(0, sequences_batch_num, bptt)):\n",
    "    # Get Batch\n",
    "    data, target = get_batch(processed_example_sequence_input, processed_example_sequence_target, i, bptt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([4, 2, 4, 5, 4, 4, 3, 4, 2, 2, 2, 4, 4, 5, 2, 4, 5, 2, 5, 5, 4, 4, 4, 4,\n",
       "         2, 2, 3, 4, 5, 3, 4, 4, 2, 4, 3], device='cuda:0'),\n",
       " tensor([4, 2, 4, 5, 4, 4, 3, 4, 2, 2, 2, 4, 4, 5, 2, 4, 5, 2, 5, 5, 4, 4, 4, 4,\n",
       "         2, 2, 3, 4, 5, 3, 4, 4, 2, 4, 3], device='cuda:0'),\n",
       " torch.Size([35]),\n",
       " torch.Size([35]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "print(sequences_batch_num)\n",
    "data, target, data.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
