# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/02 training.transcription.index.ipynb.

# %% auto 0
__all__ = ['random_state', 'make_training_index', 'get_training_index', 'make_train_test_split', 'get_sequence',
           'TranscriptionDataset', 'tokenize', 'count_transcription_tokens', 'build_vocab', 'get_vocab',
           'process_training_sequence', 'batchify_sequence', 'batchify', 'get_batch']

# %% ../../../nbs/02 training.transcription.index.ipynb 1
from pathlib import Path
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import torch
from torch import Tensor
from torch.utils.data import Dataset, IterableDataset
from torchtext.vocab import vocab, Vocab
from collections import Counter, OrderedDict
import operator

tqdm.pandas()

from ...data.download import load_config, \
    get_latest_assembly_path, get_genomic_genbank_path

random_state = 42

# %% ../../../nbs/02 training.transcription.index.ipynb 4
def make_training_index(index_dir: Path, sequences_path: Path, sample: bool = False, save: bool = False):
    training_data_index_frames = []
    sequences = list(sequences_path.glob("*.csv"))
    if sample:
        sequences = sequences[-2:]
    for f in tqdm(sequences):
        f_frame = pd.read_csv(f, usecols=["geneid"]).reset_index(drop=False).rename({"index": "file_index"}, axis=1)
        f_frame.loc[:, 'file'] = f.stem
        training_data_index_frames.append(f_frame)
    training_data_index = pd.concat(
        training_data_index_frames, 
        axis=0, ignore_index=True
    ).reset_index(drop=True)
    if save:
        training_data_index.to_csv(index_dir / "index.csv", index=False)
    return training_data_index


def get_training_index(index_dir: Path, sequences_path: Path = None, **make_kwargs):
    index_path = index_dir / "index.csv"
    if not index_path.exists():
        return make_training_index(index_dir, sequences_path, **make_kwargs)
    else:
        return pd.read_csv(index_path)

# %% ../../../nbs/02 training.transcription.index.ipynb 7
def make_train_test_split(index: pd.DataFrame, random_state = 42):
    return train_test_split(index, random_state=random_state)

# %% ../../../nbs/02 training.transcription.index.ipynb 11
def get_sequence(sequences_path: Path, file: str, idx: int) -> pd.Series:
    row = pd.read_csv(sequences_path / f"{file}.csv", header=None, skiprows=idx, nrows=1)
    return row

# %% ../../../nbs/02 training.transcription.index.ipynb 14
class TranscriptionDataset(Dataset):
    def __init__(self, training_path: Path, sequences_path: Path, train: bool):
        self.training_path = training_path
        self.sequences_path = sequences_path
        self.train = train
        self.training_index = get_training_index(self.training_path, self.sequences_path, sample=False, save=True)
        self.train_idx, self.test_idx = make_train_test_split(self.training_index)

    def __len__(self) -> int:
        if self.train:
            return self.train_idx.shape[0]
        else:
            return self.test_idx.shape[0]

    def __getitem__(self, idx) -> tuple[str, str]:
        if self.train:
            sequence_row = self.train_idx.iloc[idx, :]
        else:
            sequence_row = self.test_idx.iloc[idx, :]
        sequence_file_stem = sequence_row.file
        sequence_file_idx = sequence_row.file_index
        sequence = get_sequence(sequences_data_path, sequence_file_stem, sequence_file_idx)
        sequence_input = sequence.iloc[0, 1]
        sequence_target = sequence.iloc[0, 2]
        return sequence_input, sequence_target

# %% ../../../nbs/02 training.transcription.index.ipynb 18
def tokenize(seq: str) -> list[str]:
    return seq.split(",")

# %% ../../../nbs/02 training.transcription.index.ipynb 22
def count_transcription_tokens(genes_path: Path, pbar: bool = False) -> OrderedDict:
    token_counter = Counter()
    gene_files = list(genes_path.glob("*.csv"))
    if pbar:
        gene_files = tqdm(gene_files, leave=False, position=0)
    for f in gene_files:
        f_sequences = pd.read_csv(f, usecols=['sequence']).sequence
        if pbar:
            f_sequences = tqdm(f_sequences, leave=False, position=1)
        f_counter = sum(map(Counter, f_sequences), Counter())
        token_counter = token_counter + f_counter
    token_ordered_dict = OrderedDict(token_counter.most_common())
    return token_ordered_dict


def build_vocab(
        genes_path: Path, 
        pbar: bool = False, 
        intron_token: str = "<intron>", unknown_token: str = "<unk>", 
        default_index: int = -1):
    token_ordered_dict = count_transcription_tokens(genes_path=genes_path, pbar=pbar)
    transcription_vocab = vocab(token_ordered_dict, specials=[intron_token, unknown_token])
    transcription_vocab.set_default_index(default_index)
    return transcription_vocab


def get_vocab(
        training_path: Path,
        **build_vocab_kwargs
        ) -> Vocab:
    vocab_path = training_path / "vocab.pt"
    if not vocab_path.exists():
        transcription_vocab = build_vocab(**build_vocab_kwargs)
        torch.save(transcription_vocab, vocab_path)
    else:
        transcription_vocab = torch.load(vocab_path)
    return transcription_vocab

# %% ../../../nbs/02 training.transcription.index.ipynb 25
def process_training_sequence(sequence: tuple[str, str], data_vocab: Vocab) -> tuple[Tensor, Tensor]:
    """Converts raw text into a flat Tensor."""
    input_tensor = torch.tensor(data_vocab(tokenize(sequence[0])), dtype=torch.long)
    target_tensor = torch.tensor(data_vocab(tokenize(sequence[1])), dtype=torch.long)
    return input_tensor, target_tensor

# %% ../../../nbs/02 training.transcription.index.ipynb 29
def batchify_sequence(sequence: Tensor, bsz: int) -> Tensor:
    global device
    seq_len = sequence.size(0) // bsz
    data = sequence[:seq_len * bsz]
    data = data.view(bsz, seq_len).t().contiguous()
    return data.to(device)


def batchify(sequence_tensors: tuple[Tensor, Tensor], bsz: int) -> tuple[Tensor, Tensor]:
    """Divides the data into ``bsz`` separate sequences, removing extra elements
    that wouldn't cleanly fit.

    Arguments:
        data: Tensor, shape ``[N]``
        bsz: int, batch size

    Returns:
        Tensor of shape ``[N // bsz, bsz]``
    """
    input_batches = batchify_sequence(sequence_tensors[0], bsz)
    target_batches = batchify_sequence(sequence_tensors[1], bsz)
    return input_batches, target_batches

# %% ../../../nbs/02 training.transcription.index.ipynb 33
def get_batch(input: Tensor, target: Tensor, i: int, bptt: int = 35) -> tuple[Tensor, Tensor]:
    global device
    seq_len = min(bptt, len(input) - 1 - i)
    data = input[i:i+seq_len].to(device)
    target = target[i:i+seq_len].to(device)
    return data, target
