# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../nbs/02 training.transcription.index.ipynb.

# %% auto 0
__all__ = ['random_state', 'UNK_TOKEN', 'PAD_TOKEN', 'BOS_TOKEN', 'EOS_TOKEN', 'SPECIAL_TOKENS', 'index_training_sequence_files',
           'make_training_index', 'get_training_index', 'make_train_test_split', 'get_sequence', 'TranscriptionDataset',
           'tokenize', 'count_transcription_tokens', 'build_vocab', 'get_vocab', 'process_training_sequence',
           'batchify_sequence', 'batchify', 'get_batch']

# %% ../../../nbs/02 training.transcription.index.ipynb 1
import os
from pathlib import Path
import pandas as pd
from tqdm import tqdm
from sklearn.model_selection import train_test_split
import torch
from torch import Tensor
from torch.utils.data import Dataset
from torchtext.vocab import vocab, Vocab
from collections import Counter, OrderedDict
from multiprocessing import Pool

tqdm.pandas()

from ...data.download import load_config, \
    get_latest_assembly_path, get_genomic_genbank_path

random_state = 42

# %% ../../../nbs/02 training.transcription.index.ipynb 7
def index_training_sequence_files(sequences_path: Path) -> pd.DataFrame:
    # Index files
    chromosome_parquet_files = list(sequences_path.glob("chromosome=*/partition=*/*.parquet"))
    parquet_file_df = pd.DataFrame(chromosome_parquet_files, columns=['path'])
    # Extract partition info
    parquet_file_df_path_split = parquet_file_df.path.apply(lambda p: str(p).rsplit("/"))
    parquet_file_df.loc[:, 'chromosome'] = parquet_file_df_path_split.apply(lambda split_path: split_path[-3].split("=")[-1])
    parquet_file_df.loc[:, 'partition'] = parquet_file_df_path_split.apply(lambda split_path: int(split_path[-2].split("=")[-1]))
    # Sort
    parquet_file_df.sort_values(['chromosome', 'partition'], inplace=True)
    parquet_file_df.reset_index(drop=True, inplace=True)
    return parquet_file_df

# %% ../../../nbs/02 training.transcription.index.ipynb 8
def make_training_index(index_dir: Path, sequences_path: Path, sample: bool = False, save: bool = False):
    sequence_files = index_training_sequence_files(sequences_path)
    if sample:
        sequence_files = sequence_files.tail(2)
    sequences = sequence_files.path.tolist()
    frames = []
    for f in tqdm(sequences):
        f_frame = pd.read_parquet(f, columns=["geneid", 'transcriptid']).reset_index(drop=False).rename({"index": "file_index"}, axis=1)
        f_frame.loc[:, 'file'] = f
        frames.append(f_frame)
    training_data_index = pd.concat(
        frames, 
        axis=0, ignore_index=True
    ).reset_index(drop=True)
    if save:
        training_data_index.to_csv(index_dir / "index.csv", index=False)
    return training_data_index


def get_training_index(index_dir: Path, sequences_path: Path = None, **make_kwargs):
    index_path = index_dir / "index.csv"
    if not index_path.exists():
        return make_training_index(index_dir, sequences_path, **make_kwargs)
    else:
        return pd.read_csv(index_path)

# %% ../../../nbs/02 training.transcription.index.ipynb 11
def make_train_test_split(index: pd.DataFrame, random_state = 42):
    return train_test_split(index, random_state=random_state)

# %% ../../../nbs/02 training.transcription.index.ipynb 15
def get_sequence(file: Path, idx: int) -> pd.Series:
    row = pd.read_parquet(file).iloc[idx, :]
    return row

# %% ../../../nbs/02 training.transcription.index.ipynb 18
class TranscriptionDataset(Dataset):
    def __init__(self, training_path: Path, sequences_path: Path, train: bool):
        self.training_path = training_path
        self.sequences_path = sequences_path
        self.train = train
        self.training_index = get_training_index(self.training_path, self.sequences_path, sample=False, save=True)
        self.train_idx, self.test_idx = make_train_test_split(self.training_index)

    def __len__(self) -> int:
        if self.train:
            return self.train_idx.shape[0]
        else:
            return self.test_idx.shape[0]

    def __getitem__(self, idx) -> tuple[str, str]:
        if self.train:
            sequence_row = self.train_idx.iloc[idx, :]
        else:
            sequence_row = self.test_idx.iloc[idx, :]
        sequence = get_sequence(sequence_row.file, sequence_row.file_index)
        return sequence.input, sequence.target

# %% ../../../nbs/02 training.transcription.index.ipynb 22
def tokenize(seq: str) -> list[str]:
    return seq.split(",")

# %% ../../../nbs/02 training.transcription.index.ipynb 27
def count_transcription_tokens(parquet_path: Path) -> Counter:
    token_counter = Counter()
    sequences = pd.read_parquet(parquet_path, columns=['input', 'target'])
    input_counter = sum(sequences.input.apply(tokenize).apply(Counter).values.tolist(), Counter())
    target_counter = sum(sequences.target.apply(tokenize).apply(Counter).values.tolist(), Counter())
    token_counter = input_counter + target_counter
    return token_counter


UNK_TOKEN = "<unk>"
PAD_TOKEN = "<pad>"
BOS_TOKEN = "<bos>"
EOS_TOKEN = "<eos>"
SPECIAL_TOKENS = [
    UNK_TOKEN,
    PAD_TOKEN,
    BOS_TOKEN,
    EOS_TOKEN
]


def build_vocab(
        parquet_files: list[Path], 
        special_tokens: list[str] = SPECIAL_TOKENS,
        unknown_token: str = UNK_TOKEN):
    counter = Counter()
    max_processes = min(8, os.cpu_count() - 1)
    pool = Pool(
        processes=min(max_processes, len(parquet_files)))
    try:
        pbar = tqdm(total=len(parquet_files), leave=False)
        for c in pool.imap_unordered(count_transcription_tokens, parquet_files):
            counter = counter + c
            pbar.update(1)
    except Exception as e:
        raise e
    finally:
        pbar.close()
        pool.close()
    token_ordered_dict = OrderedDict(counter.most_common())
    transcription_vocab = vocab(token_ordered_dict, specials=special_tokens, special_first=True)
    unk_index = transcription_vocab[unknown_token]
    transcription_vocab.set_default_index(unk_index)
    return transcription_vocab


def get_vocab(
        training_path: Path,
        force_build: bool = False,
        save: bool = True,
        **build_vocab_kwargs
        ) -> Vocab:
    vocab_path = training_path / "vocab.pt"
    if not vocab_path.exists() or force_build:
        transcription_vocab = build_vocab(**build_vocab_kwargs)
        if save:
            torch.save(transcription_vocab, vocab_path)
    else:
        transcription_vocab = torch.load(vocab_path)
    return transcription_vocab

# %% ../../../nbs/02 training.transcription.index.ipynb 31
def process_training_sequence(sequence: tuple[str, str], data_vocab: Vocab) -> tuple[Tensor, Tensor]:
    """Converts raw text into a flat Tensor."""
    input_tensor = torch.tensor(data_vocab(tokenize(sequence[0])), dtype=torch.long)
    target_tensor = torch.tensor(data_vocab(tokenize(sequence[1])), dtype=torch.long)
    return input_tensor, target_tensor

# %% ../../../nbs/02 training.transcription.index.ipynb 35
def batchify_sequence(sequence: Tensor, bsz: int) -> Tensor:
    global device
    seq_len = sequence.size(0) // bsz
    data = sequence[:seq_len * bsz]
    data = data.view(bsz, seq_len).t().contiguous()
    return data.to(device)


def batchify(sequence_tensors: tuple[Tensor, Tensor], bsz: int) -> tuple[Tensor, Tensor]:
    """Divides the data into ``bsz`` separate sequences, removing extra elements
    that wouldn't cleanly fit.

    Arguments:
        data: Tensor, shape ``[N]``
        bsz: int, batch size

    Returns:
        Tensor of shape ``[N // bsz, bsz]``
    """
    input_batches = batchify_sequence(sequence_tensors[0], bsz)
    target_batches = batchify_sequence(sequence_tensors[1], bsz)
    return input_batches, target_batches

# %% ../../../nbs/02 training.transcription.index.ipynb 37
def get_batch(input: Tensor, target: Tensor, i: int, bptt: int = 35) -> tuple[Tensor, Tensor]:
    global device
    seq_len = min(bptt, len(input) - 1 - i)
    data = input[i:i+seq_len].to(device)
    target = target[i:i+seq_len].to(device)
    return data, target
