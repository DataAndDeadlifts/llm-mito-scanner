# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/archive/05_training.ipynb.

# %% auto 0
__all__ = ['yield_file_tokens', 'get_sequence_length', 'build_vocab', 'TransformersTokenizer']

# %% ../../nbs/archive/05_training.ipynb 3
from ..analysis.training import get_training_annotation_paths
from pathlib import Path
from tqdm.auto import tqdm
import pandas as pd

# %% ../../nbs/archive/05_training.ipynb 10
from torchtext.vocab import build_vocab_from_iterator
from fastai.text.core import BaseTokenizer

# %% ../../nbs/archive/05_training.ipynb 17
def yield_file_tokens(path: Path, chunksize=1024, sep=" ") -> list[str]:
    with path.open('r') as f:
        leftover_text = ""
        while True:
            token_string = f.read(chunksize)
            if token_string == "":
                break
            try:
                last_sep = token_string.rindex(" ")
                # Get our chunk of uninterrupted tokens
                token_chunk = leftover_text + token_string[:last_sep]
                # Get our list of tokenized tokens
                chunk_tokens = [tok for tok in token_chunk.split(" ") if len(tok) > 0]
                # Record the leftover string not in the token string
                leftover_text = token_string[last_sep:]
                # yield our list of tokens
                yield chunk_tokens
            except ValueError:
                # If there is no sep, capture the string
                leftover_text = leftover_text + token_string
        chunk_tokens = [tok for tok in leftover_text.split(" ") if len(tok) > 0]
        if len(chunk_tokens) > 0:
            yield chunk_tokens

# %% ../../nbs/archive/05_training.ipynb 19
# Sort file paths by size
def get_sequence_length(path: Path):
    length = 0
    for tokens in yield_file_tokens(path):
        length += len(tokens)
    return length

# %% ../../nbs/archive/05_training.ipynb 25
# def make_batches(batch_min_size: int, path_df: pd.DataFrame):
#     batches = []
#     batch = []
#     batch_size = 0
#     pbar = tqdm(total=path_df.shape[0])
#     for idx, row in path_df.iterrows():
#         row_path = Path(row.value)
#         if batch_size >= batch_min_size:
#             batches.append(batch)
#             batch = [row_path]
#             batch_size = row['size']
#         else:
#             batch.append(row_path)
#             batch_size += row['size']
#         pbar.update(1)
#     pbar.close()
#     batches.append(batch)
#     return batches

# %% ../../nbs/archive/05_training.ipynb 29
from torchtext.vocab import Vocab, vocab
from collections import Counter, OrderedDict
from multiprocessing import Pool
import os
import numpy as np
from operator import attrgetter
from functools import reduce
from operator import add
import itertools


def build_vocab(file_paths: list[Path], special_tokens: list[str] = ['<unk>']) -> Vocab:
    # Terms in the vocab will be ordered by the order they're added to this counter
    counter = Counter()
    # Add common nucleotides
    nucleotide_tag = "[N]"
    nucleotides = list("ACGT")
    nucleotide_tokens = [
        f"{nucleotide_tag}{nucleotide}" for nucleotide in nucleotides
    ]
    counter.update(nucleotide_tokens)
    # mRNA tags
    counter.update(['[intron]', '[exon]'])
    # amino acids
    amino_acids = list("ARNDCQEGHILKMFPSTWYVBZX")
    amino_acid_tag = "[A]"
    codon_pos = list(range(1, 4))
    amino_acid_tokens = [
        f"{amino_acid_tag}-{amino_acid_val}-{codon_pos_val}" for amino_acid_val, codon_pos_val in itertools.product(
            amino_acids, 
            codon_pos
        )
    ]
    counter.update(amino_acid_tokens)
    # Rare nucleotides
    rare_nucleotides = list("URYKMSWBDHVN")
    rare_nucleotide_tokens = [
        f"{nucleotide_tag}-{nucleotide}" for nucleotide in rare_nucleotides
    ]
    counter.update(rare_nucleotide_tokens)
    token_vocab = vocab(counter, specials=special_tokens)
    return token_vocab

# %% ../../nbs/archive/05_training.ipynb 38
from fastai.text.all import Transform

class TransformersTokenizer(Transform):
    def __init__(self, tokenizer, vocab: Vocab): 
        self.tokenizer = tokenizer
        self.vocab = vocab
    def encodes(self, x): 
        toks = self.tokenizer(x)
        
        #return tensor(self.tokenizer.convert_tokens_to_ids(toks))
    def decodes(self, x): return self.tokenizer.decode(x.cpu().numpy())

# %% ../../nbs/archive/05_training.ipynb 41
# def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:
#     """Converts raw text into a flat Tensor."""
#     data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]
#     return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))
