# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01 transform.pre_training.ipynb.

# %% auto 0
__all__ = ['get_latest_annotation_path', 'get_latest_feature_table_path', 'read_latest_feature_table', 'get_genomic_fasta_path',
           'extract_accession_sequence_records', 'get_accession_sequence_record', 'yield_dna_feature_sequences',
           'write_dna_feature_sequences']

# %% ../../nbs/01 transform.pre_training.ipynb 4
from pathlib import Path
from Bio import SeqIO, SeqRecord, SeqFeature, Seq
from tqdm.auto import tqdm
from yaml import safe_load
import gzip
import pandas as pd
import typing
from tqdm.notebook import tqdm

from ..data.download import load_config

# %% ../../nbs/01 transform.pre_training.ipynb 8
def get_latest_annotation_path(
        path: Path # Raw data path
        ) -> Path: # Path of the latest annotation
    "Get the latest annotation path."
    annotations = [d for d in path.iterdir() if d.is_dir()]
    annotations_df = pd.DataFrame(annotations, columns=['path'])
    annotations_df.loc[:, 'accession'] = annotations_df.path.apply(lambda p: p.name)
    annotations_df.loc[:, 'accession_prefix'] = annotations_df.accession.apply(
        lambda acc: acc.split(".", 1)[0]
    )
    annotations_df.sort_values("accession_prefix", inplace=True, ascending=False)
    return annotations_df.iloc[0, 0]

# %% ../../nbs/01 transform.pre_training.ipynb 10
def get_latest_feature_table_path(
        path: Path # Latest annotation path
        ) -> Path: # Path of the feature table
    "Return latest feature table path."
    return next(path.glob("*_feature_table.txt.gz"), None)


def read_latest_feature_table(
        path: Path # Feature table path
        ) -> pd.DataFrame: # Dataframe of features
    "Return latest feature table."
    return pd.read_csv(
        path,
        compression="gzip",
        sep='\t'
    )

# %% ../../nbs/01 transform.pre_training.ipynb 14
def get_genomic_fasta_path(
        annotation_path: Path # Annotation path,
        ) -> Path: # Fasta path
    return next(annotation_path.glob("*[!from]_genomic.fna.gz"), None)

# %% ../../nbs/01 transform.pre_training.ipynb 16
def extract_accession_sequence_records(
        gzip_fasta_path: Path,
        write_path: Path,
        expected_accessions: int = None
):
    pbar = None
    if isinstance(expected_accessions, int):
        pbar = tqdm(total=expected_accessions, ncols=80, leave=True)
    with gzip.open(str(gzip_fasta_path.resolve()), mode='rt') as f:
        for record in SeqIO.parse(f, "fasta"):
            record_write_path = write_path / f"{record.id}.fasta"
            if not record_write_path.exists():
                SeqIO.write(record, record_write_path, "fasta")
            if pbar is not None:
                pbar.update(1)
    if pbar is not None:
        pbar.close()

# %% ../../nbs/01 transform.pre_training.ipynb 18
def get_accession_sequence_record(
        accession_path: str,
) -> SeqRecord:
    with accession_path.open("r") as f:
        return next(SeqIO.parse(f, "fasta"), None)

# %% ../../nbs/01 transform.pre_training.ipynb 22
def yield_dna_feature_sequences(
        feature_table: pd.DataFrame, # Feature DataFrame
        accessions_path_parent: Path
        ) -> typing.Generator[typing.Tuple[int, str], None, None]: # Yield nucleotide strings denoting the feature
    "For each accession in the feature table, yield the index and sequence of the features."
    for accession in feature_table.genomic_accession.unique():
        accession_features = feature_table[feature_table.genomic_accession == accession]
        accession_path = accessions_path_parent / f"{accession}.fasta"
        if not accession_path.exists():
            raise FileNotFoundError(accession_path)
        accession_sequence_record = get_accession_sequence_record(accession_path)
        for idx, feature in accession_features.iterrows():
            feature_start = feature.start
            feature_end = feature.end
            feature_strand = feature.strand
            feature_interval_length = feature.feature_interval_length
            feature_sequence_record = accession_sequence_record[feature_start - 1: feature_end]
            if feature_strand == "-":
                feature_sequence_record = feature_sequence_record.reverse_complement()
            feature_sequence = feature_sequence_record.seq
            # assert len(feature_sequence) == feature_interval_length
            yield idx, str(feature_sequence)

# %% ../../nbs/01 transform.pre_training.ipynb 27
def write_dna_feature_sequences(
        feature_generator: typing.Generator[typing.Tuple[int, str], None, None], # Feature sequence generator
        file_path: Path, # Where we're writing to
        feature_num: int = None, # Number of features we expect to write, creates a progress bar
        batch_limit_len: int = 5000 # Number of features we gather before we write
        ):
    "Get features and their sequences, write to disk."
    batch_size = 0
    batch = []
    write_header = ~file_path.exists()
    pbar = None
    if isinstance(feature_num, int):
        pbar = tqdm(total=feature_num, position=0, ncols=80)
        pbar.set_description("Collecting")
    for feature_idx, feature_sequence in feature_generator:
        batch.append([feature_idx, feature_sequence])
        batch_size += 1
        if batch_size >= batch_limit_len:
            pbar.set_description("Writing")
            pd.DataFrame(
                batch,
                columns=['idx', 'sequence'],
            ).to_csv(
                file_path, 
                header=write_header, 
                index=False,
                mode="w+" if write_header else "a")
            if pbar is not None:
                pbar.update(batch_size)
                pbar.set_description("Collecting")
            write_header = False
            batch = []
            batch_size = 0
    if pbar is not None:
        pbar.set_description("Final Write")
    pd.DataFrame(
        batch,
        columns=['idx', 'sequence']
    ).to_csv(
        file_path, 
        header=write_header, 
        index=False,
        mode="w+" if write_header else "a")
    if pbar is not None:
        pbar.update(len(batch))
        pbar.close()
